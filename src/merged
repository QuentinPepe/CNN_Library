/*-- File: ConvolutionalLayer.cpp start --*/
/*-- #include "ConvolutionalLayer.h" start --*/

/*-- #include "Layer.h" start --*/

#include <memory>
#include <string>
#include <iostream>

namespace nnm {

    template<typename T>
    class Layer {
    public:
        virtual ~Layer() = default;

        virtual T forward(const T &input) = 0;

        virtual T backward(const T &input, const T &output_gradient) = 0;

        virtual void update_parameters(float learning_rate) = 0;

        virtual void save(std::ostream &os) const = 0;

        virtual void load(std::istream &is) = 0;

        virtual std::string get_name() const = 0;

        virtual size_t get_input_size() const = 0;

        virtual size_t get_output_size() const = 0;

        virtual std::unique_ptr<Layer<T>> clone() const = 0;
    };

} // namespace nnm
/*-- #include "Tensor4D.h" start --*/

#include <vector>
#include <stdexcept>
#include <algorithm>
#include <cstring>
#include <immintrin.h>
#include <iostream>
#include <cmath>
#include <numeric>
/*-- #include "Matrix.h" start --*/

#include <vector>
#include <stdexcept>
#include <algorithm>
#include <cstring>
#include <immintrin.h>
#include <iostream>
#include <cmath>
#include <numeric>

namespace nnm {
    class Matrix {
    private:
        std::vector<float> data;
        size_t rows;
        size_t cols;

        static constexpr size_t STRASSEN_THRESHOLD = 64;

        [[nodiscard]] Matrix multiplyAVX(const Matrix &other) const {
            if (cols != other.rows) {
                throw std::invalid_argument("Matrix dimensions do not match for multiplication");
            }
            Matrix result(rows, other.cols);
#pragma omp parallel for
            for (size_t i = 0; i < rows; ++i) {
                for (size_t j = 0; j < other.cols; j += 4) {
                    __m256d sum = _mm256_setzero_pd();
                    for (size_t k = 0; k < cols; ++k) {
                        __m256d a = _mm256_set1_pd(static_cast<double>((*this)(i, k)));
                        __m256d b = _mm256_setr_pd(
                                static_cast<double>(other(k, j)),
                                static_cast<double>(other(k, j + 1)),
                                static_cast<double>(other(k, j + 2)),
                                static_cast<double>(other(k, j + 3))
                        );
                        sum = _mm256_add_pd(sum, _mm256_mul_pd(a, b));
                    }
                    double temp[4];
                    _mm256_storeu_pd(temp, sum);
                    for (int k = 0; k < 4 && j + k < other.cols; ++k) {
                        result(i, j + k) = static_cast<float>(temp[k]);
                    }
                }
            }
            return result;
        }

        [[nodiscard]] Matrix strassen(const Matrix &other) const {
            if (rows != cols || other.rows != other.cols || rows != other.rows) {
                throw std::invalid_argument("Matrices must be square and of the same size for Strassen's algorithm");
            }

            size_t n = rows;
            if (n <= STRASSEN_THRESHOLD) {
                return multiplyAVX(other);
            }

            size_t new_size = n / 2;
            Matrix a11(new_size, new_size), a12(new_size, new_size), a21(new_size, new_size), a22(new_size, new_size);
            Matrix b11(new_size, new_size), b12(new_size, new_size), b21(new_size, new_size), b22(new_size, new_size);

            // Split matrices
            for (size_t i = 0; i < new_size; ++i) {
                for (size_t j = 0; j < new_size; ++j) {
                    a11(i, j) = (*this)(i, j);
                    a12(i, j) = (*this)(i, j + new_size);
                    a21(i, j) = (*this)(i + new_size, j);
                    a22(i, j) = (*this)(i + new_size, j + new_size);

                    b11(i, j) = other(i, j);
                    b12(i, j) = other(i, j + new_size);
                    b21(i, j) = other(i + new_size, j);
                    b22(i, j) = other(i + new_size, j + new_size);
                }
            }

            // Recursive steps
            Matrix p1 = (a11 + a22).strassen(b11 + b22);
            Matrix p2 = (a21 + a22).strassen(b11);
            Matrix p3 = a11.strassen(b12 - b22);
            Matrix p4 = a22.strassen(b21 - b11);
            Matrix p5 = (a11 + a12).strassen(b22);
            Matrix p6 = (a21 - a11).strassen(b11 + b12);
            Matrix p7 = (a12 - a22).strassen(b21 + b22);

            // Calculate result quadrants
            Matrix c11 = p1 + p4 - p5 + p7;
            Matrix c12 = p3 + p5;
            Matrix c21 = p2 + p4;
            Matrix c22 = p1 - p2 + p3 + p6;

            // Combine result
            Matrix result(n, n);
            for (size_t i = 0; i < new_size; ++i) {
                for (size_t j = 0; j < new_size; ++j) {
                    result(i, j) = c11(i, j);
                    result(i, j + new_size) = c12(i, j);
                    result(i + new_size, j) = c21(i, j);
                    result(i + new_size, j + new_size) = c22(i, j);
                }
            }

            return result;
        }

    public:
        Matrix(size_t rows, size_t cols) : rows(rows), cols(cols), data(rows * cols, 0.0f) {}

        Matrix(size_t rows, size_t cols, float value) : rows(rows), cols(cols), data(rows * cols, value) {}


        float &operator()(size_t i, size_t j) {
            return data[i * cols + j];
        }

        const float &operator()(size_t i, size_t j) const {
            return data[i * cols + j];
        }

        Matrix operator+(const Matrix &other) const {
            if (rows != other.rows || cols != other.cols) {
                throw std::invalid_argument("Matrix dimensions do not match for addition");
            }
            Matrix result(rows, cols);
            for (size_t i = 0; i < rows * cols; ++i) {
                result.data[i] = data[i] + other.data[i];
            }
            return result;
        }

        Matrix operator-(const Matrix &other) const {
            if (rows != other.rows || cols != other.cols) {
                throw std::invalid_argument("Matrix dimensions do not match for subtraction");
            }
            Matrix result(rows, cols);
            for (size_t i = 0; i < rows * cols; ++i) {
                result.data[i] = data[i] - other.data[i];
            }
            return result;
        }

        Matrix operator*(const Matrix &other) const {
            if (cols != other.rows) {
                throw std::invalid_argument("Matrix dimensions do not match for multiplication");
            }
            if (rows == cols && other.rows == other.cols && (rows & (rows - 1)) == 0) {
                return strassen(other);
            }
            return multiplyAVX(other);
        }

        void print() const {
            for (size_t i = 0; i < rows; ++i) {
                for (size_t j = 0; j < cols; ++j) {
                    std::cout << (*this)(i, j) << " ";
                }
                std::cout << std::endl;
            }
        }

        [[nodiscard]] Matrix transpose() const {
            Matrix result(cols, rows);
            for (size_t i = 0; i < rows; ++i) {
                for (size_t j = 0; j < cols; ++j) {
                    result(j, i) = (*this)(i, j);
                }
            }
            return result;
        }

        void reshape(size_t new_rows, size_t new_cols) {
            if (new_rows * new_cols != rows * cols) {
                throw std::invalid_argument(
                        "New dimensions must have the same number of elements as the original matrix");
            }
            rows = new_rows;
            cols = new_cols;
        }


        [[nodiscard]] Matrix pad(size_t pad_h, size_t pad_w) const {
            Matrix padded(rows + 2 * pad_h, cols + 2 * pad_w);
            for (size_t i = 0; i < rows; ++i) {
                for (size_t j = 0; j < cols; ++j) {
                    padded(i + pad_h, j + pad_w) = (*this)(i, j);
                }
            }
            return padded;
        }

        [[nodiscard]] Matrix subMatrix(size_t start_row, size_t start_col, size_t sub_rows, size_t sub_cols) const {
            Matrix sub(sub_rows, sub_cols);
            for (size_t i = 0; i < sub_rows; ++i) {
                for (size_t j = 0; j < sub_cols; ++j) {
                    sub(i, j) = (*this)(start_row + i, start_col + j);
                }
            }
            return sub;
        }

        [[nodiscard]] Matrix elementWiseMul(const Matrix &other) const {
            if (rows != other.rows || cols != other.cols) {
                throw std::invalid_argument("Matrix dimensions do not match for element-wise multiplication");
            }
            Matrix result(rows, cols);
            for (size_t i = 0; i < rows * cols; ++i) {
                result.data[i] = data[i] * other.data[i];
            }
            return result;
        }

        [[nodiscard]] float sum() const {
            return std::accumulate(data.begin(), data.end(), 0.0f);
        }


        [[nodiscard]] size_t getRows() const { return rows; }

        [[nodiscard]] size_t getCols() const { return cols; }

        const std::vector<float> &getData() const { return data; }

        std::vector<float> &getData() { return data; }

    };
} // namespace nnm

namespace nnm {
    class Tensor4D {
    private:
        std::vector<float> data;
        size_t batch_size, channels, height, width;

    public:
        Tensor4D(size_t batch_size, size_t channels, size_t height, size_t width)
                : batch_size(batch_size), channels(channels), height(height), width(width),
                  data(batch_size * channels * height * width, 0.0f) {}

        Tensor4D(size_t batch_size, size_t channels, size_t height, size_t width, float value)
                : batch_size(batch_size), channels(channels), height(height), width(width),
                  data(batch_size * channels * height * width, value) {}


        float &operator()(size_t n, size_t c, size_t h, size_t w) {
            return data[(n * channels * height * width) + (c * height * width) + (h * width) + w];
        }

        const float &operator()(size_t n, size_t c, size_t h, size_t w) const {
            return data[(n * channels * height * width) + (c * height * width) + (h * width) + w];
        }

        Tensor4D operator+(const Tensor4D &other) const {
            if (batch_size != other.batch_size || channels != other.channels ||
                height != other.height || width != other.width) {
                throw std::invalid_argument("Tensor dimensions do not match for addition");
            }
            Tensor4D result(batch_size, channels, height, width);
            for (size_t i = 0; i < data.size(); ++i) {
                result.data[i] = data[i] + other.data[i];
            }
            return result;
        }

        Tensor4D operator-(const Tensor4D &other) const {
            if (batch_size != other.batch_size || channels != other.channels ||
                height != other.height || width != other.width) {
                throw std::invalid_argument("Tensor dimensions do not match for subtraction");
            }
            Tensor4D result(batch_size, channels, height, width);
            for (size_t i = 0; i < data.size(); ++i) {
                result.data[i] = data[i] - other.data[i];
            }
            return result;
        }

        Tensor4D elementWiseMul(const Tensor4D &other) const {
            if (batch_size != other.batch_size || channels != other.channels ||
                height != other.height || width != other.width) {
                throw std::invalid_argument("Tensor dimensions do not match for element-wise multiplication");
            }
            Tensor4D result(batch_size, channels, height, width);
            for (size_t i = 0; i < data.size(); ++i) {
                result.data[i] = data[i] * other.data[i];
            }
            return result;
        }

        float sum() const {
            return std::accumulate(data.begin(), data.end(), 0.0f);
        }

        void fill(float value) {
            std::fill(data.begin(), data.end(), value);
        }

        void print() const {
            for (size_t n = 0; n < batch_size; ++n) {
                std::cout << "Batch " << n << ":\n";
                for (size_t c = 0; c < channels; ++c) {
                    std::cout << "Channel " << c << ":\n";
                    for (size_t h = 0; h < height; ++h) {
                        for (size_t w = 0; w < width; ++w) {
                            std::cout << (*this)(n, c, h, w) << " ";
                        }
                        std::cout << "\n";
                    }
                    std::cout << "\n";
                }
                std::cout << "\n";
            }
        }

        Tensor4D pad(size_t pad_h, size_t pad_w) const {
            Tensor4D padded(batch_size, channels, height + 2 * pad_h, width + 2 * pad_w);
            for (size_t n = 0; n < batch_size; ++n) {
                for (size_t c = 0; c < channels; ++c) {
                    for (size_t h = 0; h < height; ++h) {
                        for (size_t w = 0; w < width; ++w) {
                            padded(n, c, h + pad_h, w + pad_w) = (*this)(n, c, h, w);
                        }
                    }
                }
            }
            return padded;
        }

        Tensor4D subTensor(size_t start_n, size_t start_c, size_t start_h, size_t start_w,
                           size_t sub_batch, size_t sub_channels, size_t sub_height, size_t sub_width) const {
            Tensor4D sub(sub_batch, sub_channels, sub_height, sub_width);
            for (size_t n = 0; n < sub_batch; ++n) {
                for (size_t c = 0; c < sub_channels; ++c) {
                    for (size_t h = 0; h < sub_height; ++h) {
                        for (size_t w = 0; w < sub_width; ++w) {
                            sub(n, c, h, w) = (*this)(start_n + n, start_c + c, start_h + h, start_w + w);
                        }
                    }
                }
            }
            return sub;
        }

        bool operator==(const Tensor4D &other) const {
            if (getBatchSize() != other.getBatchSize() || getChannels() != other.getChannels() ||
                getHeight() != other.getHeight() || getWidth() != other.getWidth()) {
                return false;
            }
            for (size_t n = 0; n < getBatchSize(); ++n) {
                for (size_t c = 0; c < getChannels(); ++c) {
                    for (size_t h = 0; h < getHeight(); ++h) {
                        for (size_t w = 0; w < getWidth(); ++w) {
                            if ((*this)(n, c, h, w) != other(n, c, h, w)) {
                                return false;
                            }
                        }
                    }
                }
            }
            return true;
        }

        Matrix channelToMatrix(const Tensor4D &tensor, size_t batch_index, size_t channel_index) {
            size_t height = tensor.getHeight();
            size_t width = tensor.getWidth();
            Matrix result(height, width);

            size_t base_offset = (batch_index * tensor.getChannels() * height * width) +
                                 (channel_index * height * width);

            const float *tensor_data = tensor.getData().data() + base_offset;

            float *matrix_data = result.getData().data();

            size_t num_elements = height * width;

            size_t i = 0;
            for (; i + 7 < num_elements; i += 8) {
                __m256 tensor_values = _mm256_loadu_ps(tensor_data + i);
                _mm256_storeu_ps(matrix_data + i, tensor_values);
            }

            for (; i < num_elements; ++i) {
                matrix_data[i] = tensor_data[i];
            }

            return result;
        }


        [[nodiscard]] Tensor4D
        pad(const std::vector<std::pair<size_t, size_t>> &padding, float constant_value = 0.0f) const {
            if (padding.size() != 4) {
                throw std::invalid_argument("Padding should be specified for all 4 dimensions");
            }

            size_t new_batch = batch_size + padding[0].first + padding[0].second;
            size_t new_channels = channels + padding[1].first + padding[1].second;
            size_t new_height = height + padding[2].first + padding[2].second;
            size_t new_width = width + padding[3].first + padding[3].second;

            Tensor4D padded(new_batch, new_channels, new_height, new_width);
            padded.fill(constant_value);

            for (size_t n = 0; n < batch_size; ++n) {
                for (size_t c = 0; c < channels; ++c) {
                    for (size_t h = 0; h < height; ++h) {
                        for (size_t w = 0; w < width; ++w) {
                            padded(n + padding[0].first,
                                   c + padding[1].first,
                                   h + padding[2].first,
                                   w + padding[3].first) = (*this)(n, c, h, w);
                        }
                    }
                }
            }

            return padded;
        }

        size_t getBatchSize() const { return batch_size; }

        size_t getChannels() const { return channels; }

        size_t getHeight() const { return height; }

        size_t getWidth() const { return width; }

        const std::vector<float> &getData() const { return data; }

        std::vector<float> &getData() { return data; }
    };
} // namespace nnm
/*-- #include "Matrix.h" start --*/
/*-- #include "Vector.h" start --*/

#include <vector>
#include <stdexcept>
#include <algorithm>
#include <numeric>
#include <immintrin.h>
#include <cmath>

namespace nnm {

    class Vector {
    private:
        alignas(32) std::vector<float> data;

    public:
        explicit Vector(size_t size) : data(size) {}

        Vector(size_t size, float initial_value) : data(size, initial_value) {}

        Vector(std::initializer_list<float> init) : data(init) {}

        float &operator[](size_t index) {
            return data[index];
        }

        const float &operator[](size_t index) const {
            return data[index];
        }

        [[nodiscard]] size_t size() const {
            return data.size();
        }

        Vector operator+(const Vector &other) const {
            if (size() != other.size()) {
                throw std::invalid_argument("Vector sizes do not match for addition");
            }

            Vector result(size());
            size_t i = 0;

            // Use AVX2 operations for blocks of 8 elements
            for (; i + 8 <= size(); i += 8) {
                __m256 a = _mm256_loadu_ps(&data[i]);
                __m256 b = _mm256_loadu_ps(&other.data[i]);
                __m256 sum = _mm256_add_ps(a, b);
                _mm256_storeu_ps(&result.data[i], sum);
            }

            // Handle the remaining elements
            for (; i < size(); ++i) {
                result[i] = data[i] + other[i];
            }

            return result;
        }

        [[nodiscard]] float dot(const Vector &other) const {
            if (size() != other.size()) {
                throw std::invalid_argument("Vector sizes do not match for dot product");
            }

            __m256 sum = _mm256_setzero_ps();
            size_t i = 0;

            // Use AVX2 operations for blocks of 8 elements
            for (; i + 8 <= size(); i += 8) {
                __m256 a = _mm256_loadu_ps(&data[i]);
                __m256 b = _mm256_loadu_ps(&other.data[i]);
                sum = _mm256_add_ps(sum, _mm256_mul_ps(a, b));
            }

            // Sum the results from AVX2
            float partial_sum[8];
            _mm256_storeu_ps(partial_sum, sum);
            float dot_product = partial_sum[0] + partial_sum[1] + partial_sum[2] + partial_sum[3] +
                                partial_sum[4] + partial_sum[5] + partial_sum[6] + partial_sum[7];

            // Handle the remaining elements
            for (; i < size(); ++i) {
                dot_product += data[i] * other[i];
            }

            return dot_product;
        }

        [[nodiscard]] float norm() const {
            return std::sqrt(this->dot(*this));
        }

        Vector operator*(float scalar) const {
            Vector result(size());
            size_t i = 0;

            // Use AVX2 operations for blocks of 8 elements
            __m256 s = _mm256_set1_ps(scalar);
            for (; i + 8 <= size(); i += 8) {
                __m256 a = _mm256_loadu_ps(&data[i]);
                __m256 product = _mm256_mul_ps(a, s);
                _mm256_storeu_ps(&result.data[i], product);
            }

            // Handle the remaining elements
            for (; i < size(); ++i) {
                result[i] = data[i] * scalar;
            }

            return result;
        }

        void fill(float value) {
            size_t i = 0;

            // Use AVX2 operations for blocks of 8 elements
            __m256 v = _mm256_set1_ps(value);
            for (; i + 8 <= size(); i += 8) {
                _mm256_storeu_ps(&data[i], v);
            }

            // Handle the remaining elements
            for (; i < size(); ++i) {
                data[i] = value;
            }
        }
    };

} // namespace nnm

#include <random>

namespace nnm {

    class ConvolutionalLayer : public Layer<Tensor4D> {
    private:
        size_t in_channels, out_channels, kernel_size, stride, padding;
        Tensor4D weights;
        Vector bias;
        Tensor4D weight_gradients;
        Vector bias_gradients;

    public:
        ConvolutionalLayer(size_t in_channels, size_t out_channels, size_t kernel_size,
                           size_t stride = 1, size_t padding = 0);

        Tensor4D forward(const Tensor4D &input) override;

        Tensor4D backward(const Tensor4D &input, const Tensor4D &output_gradient) override;

        void update_parameters(float learning_rate) override;

        void save(std::ostream &os) const override;

        void load(std::istream &is) override;

        [[nodiscard]] std::string get_name() const override;

        [[nodiscard]] size_t get_input_size() const override;

        [[nodiscard]] size_t get_output_size() const override;

        [[nodiscard]] std::unique_ptr<Layer<Tensor4D>> clone() const override;

        void set_weights(const Tensor4D &new_weights);

        void set_bias(const Vector &new_bias);

        [[nodiscard]] const Tensor4D &get_weights() const { return weights; }

        [[nodiscard]] const Vector &get_bias() const { return bias; }

        [[nodiscard]] size_t get_padding() const { return padding; }

        [[nodiscard]] size_t get_kernel_size() const { return kernel_size; }

        [[nodiscard]] size_t get_stride() const { return stride; }
    };

} // namespace nnm
#include <cmath>
#include <stdexcept>

namespace nnm {

    ConvolutionalLayer::ConvolutionalLayer(size_t in_channels, size_t out_channels, size_t kernel_size,
                                           size_t stride, size_t padding)
            : in_channels(in_channels), out_channels(out_channels), kernel_size(kernel_size),
              stride(stride), padding(padding),
              weights(out_channels, in_channels, kernel_size, kernel_size),
              bias(out_channels),
              weight_gradients(out_channels, in_channels, kernel_size, kernel_size),
              bias_gradients(out_channels) {

        // Xavier/Glorot initialization for weights
        std::random_device rd;
        std::mt19937 gen(rd());
        float limit = std::sqrt(
                6.0f / (in_channels * kernel_size * kernel_size + out_channels * kernel_size * kernel_size));
        std::uniform_real_distribution<> dis(-limit, limit);

        for (size_t o = 0; o < out_channels; ++o) {
            for (size_t i = 0; i < in_channels; ++i) {
                for (size_t h = 0; h < kernel_size; ++h) {
                    for (size_t w = 0; w < kernel_size; ++w) {
                        weights(o, i, h, w) = static_cast<float>(dis(gen));
                    }
                }
            }
        }

        // Initialize biases to zero
        for (size_t i = 0; i < out_channels; ++i) {
            bias[i] = 0.0f;
        }

        // Initialize gradients to zero
        weight_gradients.fill(0.0f);
        bias_gradients.fill(0.0f);
    }


    Tensor4D ConvolutionalLayer::forward(const Tensor4D &input) {
        size_t N = input.getBatchSize();
        size_t H = input.getHeight();
        size_t W = input.getWidth();

        size_t H_out = 1 + (H + 2 * padding - kernel_size) / stride;
        size_t W_out = 1 + (W + 2 * padding - kernel_size) / stride;

        Tensor4D output(N, out_channels, H_out, W_out);

        Tensor4D padded_input = input.pad({{0,       0},
                                           {0,       0},
                                           {padding, padding},
                                           {padding, padding}});

        for (size_t n = 0; n < N; ++n) {
            for (size_t f = 0; f < out_channels; ++f) {
                int height_index = 0;
                for (size_t i = 0; i < H; i += stride) {
                    int width_index = 0;
                    for (size_t j = 0; j < W; j += stride) {
                        Tensor4D x_slice = padded_input.subTensor(n, 0, i, j,
                                                                  1, in_channels, kernel_size, kernel_size);
                        Tensor4D w_slice = weights.subTensor(f, 0, 0, 0,
                                                             1, in_channels, kernel_size, kernel_size);
                        float sum = x_slice.elementWiseMul(w_slice).sum() + bias[f];
                        output(n, f, height_index, width_index) = sum;
                        width_index++;
                    }
                    height_index++;
                }

            }
        }

        return output;
    }


    Tensor4D ConvolutionalLayer::backward(const Tensor4D &input, const Tensor4D &output_gradient) {
        throw std::runtime_error("Not implemented");
    }

    void ConvolutionalLayer::update_parameters(float learning_rate) {
        // Implement update_parameters functionality
    }

    void ConvolutionalLayer::save(std::ostream &os) const {
        // Implement save functionality
    }

    void ConvolutionalLayer::load(std::istream &is) {
        // Implement load functionality
    }

    std::string ConvolutionalLayer::get_name() const {
        return "ConvolutionalLayer";
    }

    size_t ConvolutionalLayer::get_input_size() const {
        return in_channels;
    }

    size_t ConvolutionalLayer::get_output_size() const {
        return out_channels;
    }

    void ConvolutionalLayer::set_weights(const Tensor4D &new_weights) {
        weights = new_weights;
    }

    void ConvolutionalLayer::set_bias(const Vector &new_bias) {
        bias = new_bias;
    }

    std::unique_ptr<Layer<Tensor4D>> ConvolutionalLayer::clone() const {
        return std::make_unique<ConvolutionalLayer>(*this);
    }


} // namespace nnm

/*-- File: ConvolutionalLayer.cpp end --*/
/*-- #include "ConvolutionalLayer.h" start --*/
/*-- #include "Layer.h" start --*/
/*-- #include "Matrix.h" start --*/
/*-- #include "Tensor4D.h" start --*/
/*-- #include "Vector.h" start --*/
/*-- File: main.cpp start --*/
#include <iostream>
#include <string>
#include <vector>
#include <algorithm>
#include <random>
#include <ctime>
#include <cmath>
#include <memory>

using namespace std;

enum Action {
    UP, RIGHT, DOWN, LEFT
};

class MiniGame {
public:
    virtual void reset(mt19937 &rng) = 0;

    virtual string getGPU() = 0;

    virtual vector<int> getRegisters() = 0;

    virtual void tick(const vector<Action> &actions) = 0;

    virtual bool isGameOver() = 0;

    virtual vector<int> getRankings() = 0;

    virtual string getName() = 0;
};

class HurdleRace : public MiniGame {
private:
    string map;
    vector<int> positions;
    vector<int> stunTimers;
    vector<bool> dead;
    vector<bool> jumped;
    vector<int> finished;
    int rank;

    const int STUN_DURATION = 2;

public:
    HurdleRace() : positions(3), stunTimers(3), dead(3, false), jumped(3, false), finished(3, -1), rank(0) {}

    void reset(mt19937 &rng) override {
        uniform_int_distribution<> distStart(3, 7);
        uniform_int_distribution<> distHurdles(3, 6);
        uniform_int_distribution<> distBool(0, 1);

        int startStretch = distStart(rng);
        int hurdles = distHurdles(rng);
        int length = 30;

        map.clear();
        map.append(startStretch, '.');
        for (int i = 0; i < hurdles; ++i) {
            map += distBool(rng) ? "#...." : "#...";
        }
        map.append(length - map.length(), '.');
        map.back() = '.';

        fill(positions.begin(), positions.end(), 0);
        fill(stunTimers.begin(), stunTimers.end(), 0);
        fill(finished.begin(), finished.end(), -1);
        fill(dead.begin(), dead.end(), false);
        fill(jumped.begin(), jumped.end(), false);
        rank = 0;
    }

    string getGPU() override { return map; }

    vector<int> getRegisters() override {
        vector<int> registers(8, -1);
        copy(positions.begin(), positions.end(), registers.begin());
        copy(stunTimers.begin(), stunTimers.end(), registers.begin() + 3);
        return registers;
    }

    void tick(const vector<Action> &actions) override {
        int maxX = map.length() - 1;
        int countFinishes = 0;

        for (int i = 0; i < 3; ++i) {
            jumped[i] = false;

            if (actions[i] == Action::UP) stunTimers[i] = max(0, stunTimers[i] - 1);
            if (stunTimers[i] > 0 || finished[i] > -1) continue;

            int moveBy = 0;
            bool jump = false;

            switch (actions[i]) {
                case Action::DOWN:
                    moveBy = 2;
                    break;
                case Action::LEFT:
                    moveBy = 1;
                    break;
                case Action::RIGHT:
                    moveBy = 3;
                    break;
                case Action::UP:
                    moveBy = 2;
                    jump = true;
                    jumped[i] = true;
                    break;
            }

            for (int x = 0; x < moveBy; ++x) {
                positions[i] = min(maxX, positions[i] + 1);
                if (map[positions[i]] == '#' && !jump) {
                    stunTimers[i] = STUN_DURATION;
                    break;
                }
                if (positions[i] == maxX && finished[i] == -1) {
                    finished[i] = rank;
                    countFinishes++;
                    break;
                }
                jump = false;
            }
        }
        rank += countFinishes;
    }

    bool isGameOver() override {
        int count = 0;
        for (int i = 0; i < 3; ++i) {
            if (finished[i] > -1) return true;
            if (finished[i] > -1 || dead[i]) count++;
        }
        return count >= 2;
    }

    vector<int> getRankings() override {
        vector<int> rankings(3);
        for (int i = 0; i < 3; ++i) {
            rankings[i] = (finished[i] == -1) ? rank : finished[i];
        }
        return rankings;
    }

    string getName() override { return "Hurdle Race"; }
};

class Archery : public MiniGame {
private:
    vector<vector<int>> cursors;
    vector<int> wind;
    vector<bool> dead;

public:
    Archery() : cursors(3, vector<int>(2)), dead(3, false) {}

    void reset(mt19937 &rng) override {
        uniform_int_distribution<> distPos(5, 9);
        uniform_int_distribution<> distSign(0, 1);
        uniform_int_distribution<> distRounds(12, 15);
        uniform_int_distribution<> distWind(0, 9);

        int x = distPos(rng) * (distSign(rng) ? 1 : -1);
        int y = distPos(rng) * (distSign(rng) ? 1 : -1);
        for (auto &cursor: cursors) {
            cursor[0] = x;
            cursor[1] = y;
        }

        wind.clear();
        int rounds = distRounds(rng);
        for (int i = 0; i < rounds; ++i) {
            wind.push_back(distWind(rng));
        }

        fill(dead.begin(), dead.end(), false);
    }

    string getGPU() override {
        string gpu;
        for (int w: wind) gpu += to_string(w);
        return gpu;
    }

    vector<int> getRegisters() override {
        vector<int> registers(8, -1);
        for (int i = 0; i < 3; ++i) {
            registers[i * 2] = cursors[i][0];
            registers[i * 2 + 1] = cursors[i][1];
        }
        return registers;
    }

    void tick(const vector<Action> &actions) override {
        int offset = wind[0];
        for (int i = 0; i < 3; ++i) {
            if (actions[i] == Action::UP) {
                dead[i] = true;
                continue;
            }

            int dx = 0, dy = 0;
            switch (actions[i]) {
                case Action::DOWN:
                    dy = offset;
                    break;
                case Action::LEFT:
                    dx = -offset;
                    break;
                case Action::RIGHT:
                    dx = offset;
                    break;
                case Action::UP:
                    dy = -offset;
                    break;
            }

            cursors[i][0] = clamp(cursors[i][0] + dx, -20, 20);
            cursors[i][1] = clamp(cursors[i][1] + dy, -20, 20);
        }
        wind.erase(wind.begin());
    }

    bool isGameOver() override { return wind.empty(); }

    vector<int> getRankings() override {
        vector<pair<int, double>> scores;
        for (int i = 0; i < 3; ++i) {
            double distance = sqrt(pow(cursors[i][0], 2) + pow(cursors[i][1], 2));
            scores.emplace_back(i, dead[i] ? numeric_limits<double>::max() : distance);
        }
        sort(scores.begin(), scores.end(), [](const auto &a, const auto &b) { return a.second < b.second; });

        vector<int> rankings(3);
        for (int i = 0; i < 3; ++i) {
            rankings[scores[i].first] = i;
        }
        return rankings;
    }

    string getName() override { return "Archery"; }
};

class Diving : public MiniGame {
private:
    string goal;
    vector<int> points;
    vector<int> combo;
    vector<bool> dead;
    int turnsRemaining;

public:
    Diving() : points(3, 0), combo(3, 0), dead(3, false), turnsRemaining(0) {}

    void reset(mt19937 &rng) override {
        uniform_int_distribution<> distLength(12, 15);
        uniform_int_distribution<> distAction(0, 3);

        int length = distLength(rng);
        goal.clear();
        for (int i = 0; i < length; ++i) {
            goal += "UDLR"[distAction(rng)];
        }

        fill(points.begin(), points.end(), 0);
        fill(combo.begin(), combo.end(), 0);
        fill(dead.begin(), dead.end(), false);
        turnsRemaining = length + 1;
    }

    string getGPU() override { return goal; }

    vector<int> getRegisters() override {
        vector<int> registers(8, -1);
        copy(points.begin(), points.end(), registers.begin());
        copy(combo.begin(), combo.end(), registers.begin() + 3);
        return registers;
    }

    void tick(const vector<Action> &actions) override {
        for (int i = 0; i < 3; ++i) {
            if (actions[i] == Action::UP) {
                dead[i] = true;
                continue;
            }

            if (goal[0] == "UDLR"[static_cast<int>(actions[i])]) {
                combo[i]++;
                points[i] += combo[i];
            } else {
                combo[i] = 0;
            }
        }
        goal.erase(0, 1);
        turnsRemaining--;
    }

    bool isGameOver() override { return goal.empty(); }

    vector<int> getRankings() override {
        vector<pair<int, int>> scores;
        for (int i = 0; i < 3; ++i) {
            scores.emplace_back(i, dead[i] ? -1 : points[i]);
        }
        sort(scores.begin(), scores.end(), [](const auto &a, const auto &b) { return a.second > b.second; });

        vector<int> rankings(3);
        for (int i = 0; i < 3; ++i) {
            rankings[scores[i].first] = i;
        }
        return rankings;
    }

    string getName() override { return "Diving"; }
};

class RollerSpeedSkating : public MiniGame {
private:
    vector<int> positions;
    vector<int> risk;
    vector<bool> dead;
    vector<Action> directions;
    int length, timer;

public:
    RollerSpeedSkating() : positions(3, 0), risk(3, 0), dead(3, false), length(10), timer(15) {
        directions = {Action::UP, Action::DOWN, Action::LEFT, Action::RIGHT};
    }

    void reset(mt19937 &rng) override {
        fill(positions.begin(), positions.end(), 0);
        fill(risk.begin(), risk.end(), 0);
        fill(dead.begin(), dead.end(), false);
        shuffle(directions.begin(), directions.end(), rng);
        timer = 15;
    }

    string getGPU() override {
        string gpu;
        for (Action dir: directions) {
            gpu += "UDLR"[static_cast<int>(dir)];
        }
        return gpu;
    }

    vector<int> getRegisters() override {
        vector<int> registers(8, -1);
        copy(positions.begin(), positions.end(), registers.begin());
        copy(risk.begin(), risk.end(), registers.begin() + 3);
        registers[6] = timer;
        return registers;
    }

    void tick(const vector<Action> &actions) override {
        for (int i = 0; i < 3; ++i) {
            if (actions[i] == Action::UP) {
                dead[i] = true;
                continue;
            }

            if (risk[i] < 0) {
                risk[i]++;
                continue;
            }

            int idx = find(directions.begin(), directions.end(), actions[i]) - directions.begin();
            int dx = (idx == 0) ? 1 : (idx == 3) ? 3 : 2;

            positions[i] += dx;
            int riskValue = -1 + idx;
            risk[i] = max(0, risk[i] + riskValue);
        }

        for (int i = 0; i < 3; ++i) {
            if (risk[i] < 0) continue;

            bool clash = false;
            for (int k = 0; k < 3; ++k) {
                if (k == i) continue;
                if (positions[k] % length == positions[i] % length) {
                    clash = true;
                    break;
                }
            }
            if (clash) risk[i] += 2;

            if (risk[i] >= 5) risk[i] = -2; // stun
        }

        shuffle(directions.begin(), directions.end(), mt19937(time(0)));
        timer--;
    }

    bool isGameOver() override { return timer <= 0; }

    vector<int> getRankings() override {
        vector<pair<int, int>> scores;
        for (int i = 0; i < 3; ++i) {
            scores.emplace_back(i, dead[i] ? -1 : positions[i]);
        }
        sort(scores.begin(), scores.end(), [](const auto &a, const auto &b) { return a.second > b.second; });

        vector<int> rankings(3);
        for (int i = 0; i < 3; ++i) {
            rankings[scores[i].first] = i;
        }
        return rankings;
    }

    string getName() override { return "Roller Speed Skating"; }
};

class Game {
private:
    int playerIdx;
    vector<unique_ptr<MiniGame>> games;
    vector<vector<int>> medals;
    mt19937 rng;

public:
    Game(int playerIdx) : playerIdx(playerIdx), medals(4, vector<int>(3, 0)), rng(time(0)) {
        games.push_back(make_unique<HurdleRace>());
        games.push_back(make_unique<Archery>());
        games.push_back(make_unique<Diving>());
        games.push_back(make_unique<RollerSpeedSkating>());

        for (auto &game: games) {
            game->reset(rng);
        }
    }

    void run() {
        while (true) {
            vector<string> scoreInfo(3);
            for (int i = 0; i < 3; i++) {
                int score = 1;
                for (int j = 0; j < 4; j++) {
                    score *= medals[j][1] + medals[j][0] * 3;
                }
                scoreInfo[i] = to_string(score);
                for (int j = 0; j < 4; j++) {
                    scoreInfo[i] += " " + to_string(medals[j][0]) + " " + to_string(medals[j][1]) + " " +
                                    to_string(medals[j][2]);
                }
            }

            for (const auto &info: scoreInfo) {
                cout << info << endl;
            }

            bool allGameOver = true;
            for (const auto &game: games) {
                cout << game->getGPU();
                auto registers = game->getRegisters();
                for (int reg: registers) {
                    cout << " " << reg;
                }
                cout << endl;

                if (!game->isGameOver()) {
                    allGameOver = false;
                }
            }

            if (allGameOver) {
                break;
            }

            string action;
            cin >> action;

            vector<Action> actions(3);
            if (action == "UP") actions[playerIdx] = Action::UP;
            else if (action == "DOWN") actions[playerIdx] = Action::DOWN;
            else if (action == "LEFT") actions[playerIdx] = Action::LEFT;
            else if (action == "RIGHT") actions[playerIdx] = Action::RIGHT;

            for (int i = 0; i < 3; i++) {
                if (i != playerIdx) {
                    actions[i] = static_cast<Action>(rand() % 4);
                }
            }

            for (auto &game: games) {
                if (!game->isGameOver()) {
                    game->tick(actions);
                    if (game->isGameOver()) {
                        auto rankings = game->getRankings();
                        for (int i = 0; i < 3; i++) {
                            medals[&game - &games[0]][rankings[i]]++;
                        }
                    }
                }
            }
        }
    }
};

int main() {
    int playerIdx, nbGames;
    cin >> playerIdx >> nbGames;

    Game game(playerIdx);
    game.run();

    return 0;
}
/*-- File: main.cpp end --*/
