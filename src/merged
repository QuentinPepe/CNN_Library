/*-- #include "BatchNorm2d.h" start --*/

/*-- #include "Layer.h" start --*/
#pragma GCC push_options
#pragma GCC target("avx2")
#pragma GCC optimize("O3")


#include <memory>
#include <string>
#include <iostream>

namespace nnm {

    template<typename InputType, typename OutputType>
    class Layer {
    public:
        virtual ~Layer() = default;

        virtual OutputType forward(const InputType &input) = 0;

        virtual std::string get_name() const = 0;

        virtual size_t get_input_size() const = 0;

        virtual size_t get_output_size() const = 0;
    };

} // namespace nnm
/*-- #include "Tensor4D.h" start --*/

#include <vector>
#include <stdexcept>
#include <algorithm>
#include <cstring>
#include <immintrin.h>
#include <iostream>
#include <cmath>
#include <numeric>
/*-- #include "Matrix.h" start --*/

#include <vector>
#include <stdexcept>
#include <algorithm>
#include <cstring>
#include <immintrin.h>
#include <iostream>
#include <cmath>
#include <numeric>
/*-- #include "Vector.h" start --*/

#include <vector>
#include <stdexcept>
#include <algorithm>
#include <numeric>
#include <immintrin.h>
#include <cmath>

namespace nnm {

    class Vector {
    private:
        alignas(32) std::vector<float> data;

    public:
        explicit Vector(size_t size) : data(size) {}

        Vector(size_t size, float initial_value) : data(size, initial_value) {}

        Vector(std::initializer_list<float> init) : data(init) {}

        float &operator[](size_t index) {
            return data[index];
        }

        const float &operator[](size_t index) const {
            return data[index];
        }

        [[nodiscard]] size_t size() const {
            return data.size();
        }

        Vector operator+(const Vector &other) const {
            if (size() != other.size()) {
                throw std::invalid_argument("Vector sizes do not match for addition");
            }

            Vector result(size());
            size_t i = 0;

            // Use AVX2 operations for blocks of 8 elements
            for (; i + 8 <= size(); i += 8) {
                __m256 a = _mm256_loadu_ps(&data[i]);
                __m256 b = _mm256_loadu_ps(&other.data[i]);
                __m256 sum = _mm256_add_ps(a, b);
                _mm256_storeu_ps(&result.data[i], sum);
            }

            // Handle the remaining elements
            for (; i < size(); ++i) {
                result[i] = data[i] + other[i];
            }

            return result;
        }

        [[nodiscard]] float dot(const Vector &other) const {
            if (size() != other.size()) {
                throw std::invalid_argument("Vector sizes do not match for dot product");
            }

            __m256 sum = _mm256_setzero_ps();
            size_t i = 0;

            // Use AVX2 operations for blocks of 8 elements
            for (; i + 8 <= size(); i += 8) {
                __m256 a = _mm256_loadu_ps(&data[i]);
                __m256 b = _mm256_loadu_ps(&other.data[i]);
                sum = _mm256_add_ps(sum, _mm256_mul_ps(a, b));
            }

            // Sum the results from AVX2
            float partial_sum[8];
            _mm256_storeu_ps(partial_sum, sum);
            float dot_product = partial_sum[0] + partial_sum[1] + partial_sum[2] + partial_sum[3] +
                                partial_sum[4] + partial_sum[5] + partial_sum[6] + partial_sum[7];

            // Handle the remaining elements
            for (; i < size(); ++i) {
                dot_product += data[i] * other[i];
            }

            return dot_product;
        }

        [[nodiscard]] float norm() const {
            return std::sqrt(this->dot(*this));
        }

        Vector operator*(float scalar) const {
            Vector result(size());
            size_t i = 0;

            // Use AVX2 operations for blocks of 8 elements
            __m256 s = _mm256_set1_ps(scalar);
            for (; i + 8 <= size(); i += 8) {
                __m256 a = _mm256_loadu_ps(&data[i]);
                __m256 product = _mm256_mul_ps(a, s);
                _mm256_storeu_ps(&result.data[i], product);
            }

            // Handle the remaining elements
            for (; i < size(); ++i) {
                result[i] = data[i] * scalar;
            }

            return result;
        }

        void fill(float value) {
            size_t i = 0;

            // Use AVX2 operations for blocks of 8 elements
            __m256 v = _mm256_set1_ps(value);
            for (; i + 8 <= size(); i += 8) {
                _mm256_storeu_ps(&data[i], v);
            }

            // Handle the remaining elements
            for (; i < size(); ++i) {
                data[i] = value;
            }
        }

    };

} // namespace nnm


namespace nnm {
    class Matrix {
    private:
        std::vector<float> data;
        size_t rows;
        size_t cols;

        static constexpr size_t STRASSEN_THRESHOLD = 64;

        [[nodiscard]] Matrix multiplyAVX(const Matrix &other) const {
            if (cols != other.rows) {
                throw std::invalid_argument("Matrix dimensions do not match for multiplication");
            }
            Matrix result(rows, other.cols);
#pragma omp parallel for
            for (size_t i = 0; i < rows; ++i) {
                for (size_t j = 0; j < other.cols; j += 4) {
                    __m256d sum = _mm256_setzero_pd();
                    for (size_t k = 0; k < cols; ++k) {
                        __m256d a = _mm256_set1_pd(static_cast<double>((*this)(i, k)));
                        __m256d b = _mm256_setr_pd(
                                static_cast<double>(other(k, j)),
                                static_cast<double>(other(k, j + 1)),
                                static_cast<double>(other(k, j + 2)),
                                static_cast<double>(other(k, j + 3))
                        );
                        sum = _mm256_add_pd(sum, _mm256_mul_pd(a, b));
                    }
                    double temp[4];
                    _mm256_storeu_pd(temp, sum);
                    for (int k = 0; k < 4 && j + k < other.cols; ++k) {
                        result(i, j + k) = static_cast<float>(temp[k]);
                    }
                }
            }
            return result;
        }

        [[nodiscard]] Matrix strassen(const Matrix &other) const {
            if (rows != cols || other.rows != other.cols || rows != other.rows) {
                throw std::invalid_argument("Matrices must be square and of the same size for Strassen's algorithm");
            }

            size_t n = rows;
            if (n <= STRASSEN_THRESHOLD) {
                return multiplyAVX(other);
            }

            size_t new_size = n / 2;
            Matrix a11(new_size, new_size), a12(new_size, new_size), a21(new_size, new_size), a22(new_size, new_size);
            Matrix b11(new_size, new_size), b12(new_size, new_size), b21(new_size, new_size), b22(new_size, new_size);

            // Split matrices
            for (size_t i = 0; i < new_size; ++i) {
                for (size_t j = 0; j < new_size; ++j) {
                    a11(i, j) = (*this)(i, j);
                    a12(i, j) = (*this)(i, j + new_size);
                    a21(i, j) = (*this)(i + new_size, j);
                    a22(i, j) = (*this)(i + new_size, j + new_size);

                    b11(i, j) = other(i, j);
                    b12(i, j) = other(i, j + new_size);
                    b21(i, j) = other(i + new_size, j);
                    b22(i, j) = other(i + new_size, j + new_size);
                }
            }

            // Recursive steps
            Matrix p1 = (a11 + a22).strassen(b11 + b22);
            Matrix p2 = (a21 + a22).strassen(b11);
            Matrix p3 = a11.strassen(b12 - b22);
            Matrix p4 = a22.strassen(b21 - b11);
            Matrix p5 = (a11 + a12).strassen(b22);
            Matrix p6 = (a21 - a11).strassen(b11 + b12);
            Matrix p7 = (a12 - a22).strassen(b21 + b22);

            // Calculate result quadrants
            Matrix c11 = p1 + p4 - p5 + p7;
            Matrix c12 = p3 + p5;
            Matrix c21 = p2 + p4;
            Matrix c22 = p1 - p2 + p3 + p6;

            // Combine result
            Matrix result(n, n);
            for (size_t i = 0; i < new_size; ++i) {
                for (size_t j = 0; j < new_size; ++j) {
                    result(i, j) = c11(i, j);
                    result(i, j + new_size) = c12(i, j);
                    result(i + new_size, j) = c21(i, j);
                    result(i + new_size, j + new_size) = c22(i, j);
                }
            }

            return result;
        }

    public:
        Matrix(size_t rows, size_t cols) : rows(rows), cols(cols), data(rows * cols, 0.0f) {}

        Matrix(size_t rows, size_t cols, float value) : rows(rows), cols(cols), data(rows * cols, value) {}


        float &operator()(size_t i, size_t j) {
            return data[i * cols + j];
        }

        const float &operator()(size_t i, size_t j) const {
            return data[i * cols + j];
        }

        Matrix operator+(const Matrix &other) const {
            if (rows != other.rows || cols != other.cols) {
                throw std::invalid_argument("Matrix dimensions do not match for addition");
            }
            Matrix result(rows, cols);
            for (size_t i = 0; i < rows * cols; ++i) {
                result.data[i] = data[i] + other.data[i];
            }
            return result;
        }

        Matrix operator-(const Matrix &other) const {
            if (rows != other.rows || cols != other.cols) {
                throw std::invalid_argument("Matrix dimensions do not match for subtraction");
            }
            Matrix result(rows, cols);
            for (size_t i = 0; i < rows * cols; ++i) {
                result.data[i] = data[i] - other.data[i];
            }
            return result;
        }

        Matrix operator*(const Matrix &other) const {
            if (cols != other.rows) {
                throw std::invalid_argument("Matrix dimensions do not match for multiplication");
            }
            if (rows == cols && other.rows == other.cols && (rows & (rows - 1)) == 0) {
                return strassen(other);
            }
            return multiplyAVX(other);
        }

        void print() const {
            for (size_t i = 0; i < rows; ++i) {
                for (size_t j = 0; j < cols; ++j) {
                    std::cout << (*this)(i, j) << " ";
                }
                std::cout << std::endl;
            }
        }

        [[nodiscard]] Matrix transpose() const {
            Matrix result(cols, rows);
            for (size_t i = 0; i < rows; ++i) {
                for (size_t j = 0; j < cols; ++j) {
                    result(j, i) = (*this)(i, j);
                }
            }
            return result;
        }

        void reshape(size_t new_rows, size_t new_cols) {
            if (new_rows * new_cols != rows * cols) {
                throw std::invalid_argument(
                        "New dimensions must have the same number of elements as the original matrix");
            }
            rows = new_rows;
            cols = new_cols;
        }


        [[nodiscard]] Matrix pad(size_t pad_h, size_t pad_w) const {
            Matrix padded(rows + 2 * pad_h, cols + 2 * pad_w);
            for (size_t i = 0; i < rows; ++i) {
                for (size_t j = 0; j < cols; ++j) {
                    padded(i + pad_h, j + pad_w) = (*this)(i, j);
                }
            }
            return padded;
        }

        [[nodiscard]] Matrix subMatrix(size_t start_row, size_t start_col, size_t sub_rows, size_t sub_cols) const {
            Matrix sub(sub_rows, sub_cols);
            for (size_t i = 0; i < sub_rows; ++i) {
                for (size_t j = 0; j < sub_cols; ++j) {
                    sub(i, j) = (*this)(start_row + i, start_col + j);
                }
            }
            return sub;
        }

        [[nodiscard]] Matrix elementWiseMul(const Matrix &other) const {
            if (rows != other.rows || cols != other.cols) {
                throw std::invalid_argument("Matrix dimensions do not match for element-wise multiplication");
            }
            Matrix result(rows, cols);
            for (size_t i = 0; i < rows * cols; ++i) {
                result.data[i] = data[i] * other.data[i];
            }
            return result;
        }

        [[nodiscard]] float sum() const {
            return std::accumulate(data.begin(), data.end(), 0.0f);
        }


        [[nodiscard]] size_t getRows() const { return rows; }

        [[nodiscard]] size_t getCols() const { return cols; }

        const std::vector<float> &getData() const { return data; }

        std::vector<float> &getData() { return data; }

        Vector operator*(const Vector &vec) const {
            if (cols != vec.size()) {
                throw std::invalid_argument("Matrix and vector dimensions do not match for multiplication");
            }

            Vector result(rows);
            for (size_t i = 0; i < rows; ++i) {
                float sum = 0.0f;
                for (size_t j = 0; j < cols; ++j) {
                    sum += (*this)(i, j) * vec[j];
                }
                result[i] = sum;
            }
            return result;
        }

    };
} // namespace nnm

namespace nnm {
    class Tensor4D {
    private:
        std::vector<float> data;
        size_t batch_size, channels, height, width;

    public:
        Tensor4D(size_t batch_size, size_t channels, size_t height, size_t width)
                : batch_size(batch_size), channels(channels), height(height), width(width),
                  data(batch_size * channels * height * width, 0.0f) {}

        Tensor4D(size_t batch_size, size_t channels, size_t height, size_t width, float value)
                : batch_size(batch_size), channels(channels), height(height), width(width),
                  data(batch_size * channels * height * width, value) {}

        Tensor4D(const std::vector<std::vector<std::vector<std::vector<float>>>> &values) {
            size_t batch_size = values.size();
            size_t channels = values[0].size();
            size_t height = values[0][0].size();
            size_t width = values[0][0][0].size();

            data.resize(batch_size * channels * height * width);
            for (size_t n = 0; n < batch_size; ++n) {
                for (size_t c = 0; c < channels; ++c) {
                    for (size_t h = 0; h < height; ++h) {
                        for (size_t w = 0; w < width; ++w) {
                            (*this)(n, c, h, w) = values[n][c][h][w];
                        }
                    }
                }
            }

            this->batch_size = batch_size;
            this->channels = channels;
            this->height = height;
            this->width = width;
        }

        Tensor4D(
                std::initializer_list<std::initializer_list<std::initializer_list<std::initializer_list<float>>>> values)
                : batch_size(values.size()),
                  channels(values.begin()->size()),
                  height(values.begin()->begin()->size()),
                  width(values.begin()->begin()->begin()->size()) {
            data.resize(batch_size * channels * height * width);
            size_t n = 0;
            for (const auto &batch: values) {
                size_t c = 0;
                for (const auto &channel: batch) {
                    size_t h = 0;
                    for (const auto &row: channel) {
                        size_t w = 0;
                        for (float val: row) {
                            (*this)(n, c, h, w) = val;
                            ++w;
                        }
                        ++h;
                    }
                    ++c;
                }
                ++n;
            }
        }

        Tensor4D(std::initializer_list<float> values)
                : batch_size(1), channels(values.size()), height(1), width(1) {
            data.assign(values.begin(), values.end());
        }


        float &operator()(size_t n, size_t c, size_t h, size_t w) {
            return data[(n * channels * height * width) + (c * height * width) + (h * width) + w];
        }

        const float &operator()(size_t n, size_t c, size_t h, size_t w) const {
            return data[(n * channels * height * width) + (c * height * width) + (h * width) + w];
        }

        Tensor4D operator+(const Tensor4D &other) const {
            if (batch_size != other.batch_size || channels != other.channels ||
                height != other.height || width != other.width) {
                throw std::invalid_argument("Tensor4D.cpp dimensions do not match for addition");
            }
            Tensor4D result(batch_size, channels, height, width);
            for (size_t i = 0; i < data.size(); ++i) {
                result.data[i] = data[i] + other.data[i];
            }

            return result;
        }

        Tensor4D operator-(const Tensor4D &other) const {
            if (batch_size != other.batch_size || channels != other.channels ||
                height != other.height || width != other.width) {
                throw std::invalid_argument("Tensor4D.cpp dimensions do not match for subtraction");
            }
            Tensor4D result(batch_size, channels, height, width);
            for (size_t i = 0; i < data.size(); ++i) {
                result.data[i] = data[i] - other.data[i];
            }
            return result;
        }

        Tensor4D elementWiseMul(const Tensor4D &other) const {
            if (batch_size != other.batch_size || channels != other.channels ||
                height != other.height || width != other.width) {
                throw std::invalid_argument("Tensor4D.cpp dimensions do not match for element-wise multiplication");
            }
            Tensor4D result(batch_size, channels, height, width);
            for (size_t i = 0; i < data.size(); ++i) {
                result.data[i] = data[i] * other.data[i];
            }
            return result;
        }

        float sum() const {
            return std::accumulate(data.begin(), data.end(), 0.0f);
        }

        float max() const {
            return *std::max_element(data.begin(), data.end());
        }

        float mean() const {
            return std::accumulate(data.begin(), data.end(), 0.0f) / static_cast<float>(data.size());
        }

        void fill(float value) {
            std::fill(data.begin(), data.end(), value);
        }

        void print() const {
            for (size_t n = 0; n < batch_size; ++n) {
                std::cout << "Batch " << n << ":\n";
                for (size_t c = 0; c < channels; ++c) {
                    std::cout << "Channel " << c << ":\n";
                    for (size_t h = 0; h < height; ++h) {
                        for (size_t w = 0; w < width; ++w) {
                            std::cout << (*this)(n, c, h, w) << " ";
                        }
                        std::cout << "\n";
                    }
                    std::cout << "\n";
                }
                std::cout << "\n";
            }
        }

        Tensor4D pad(size_t pad_h, size_t pad_w) const {
            Tensor4D padded(batch_size, channels, height + 2 * pad_h, width + 2 * pad_w);
            for (size_t n = 0; n < batch_size; ++n) {
                for (size_t c = 0; c < channels; ++c) {
                    for (size_t h = 0; h < height; ++h) {
                        for (size_t w = 0; w < width; ++w) {
                            padded(n, c, h + pad_h, w + pad_w) = (*this)(n, c, h, w);
                        }
                    }
                }
            }
            return padded;
        }

        Tensor4D subTensor(size_t start_n, size_t start_c, size_t start_h, size_t start_w,
                           size_t sub_batch, size_t sub_channels, size_t sub_height, size_t sub_width) const {
            Tensor4D sub(sub_batch, sub_channels, sub_height, sub_width);
            for (size_t n = 0; n < sub_batch; ++n) {
                for (size_t c = 0; c < sub_channels; ++c) {
                    for (size_t h = 0; h < sub_height; ++h) {
                        for (size_t w = 0; w < sub_width; ++w) {
                            sub(n, c, h, w) = (*this)(start_n + n, start_c + c, start_h + h, start_w + w);
                        }
                    }
                }
            }
            return sub;
        }

        bool operator==(const Tensor4D &other) const {
            if (getBatchSize() != other.getBatchSize() || getChannels() != other.getChannels() ||
                getHeight() != other.getHeight() || getWidth() != other.getWidth()) {
                return false;
            }
            for (size_t n = 0; n < getBatchSize(); ++n) {
                for (size_t c = 0; c < getChannels(); ++c) {
                    for (size_t h = 0; h < getHeight(); ++h) {
                        for (size_t w = 0; w < getWidth(); ++w) {
                            if ((*this)(n, c, h, w) != other(n, c, h, w)) {
                                return false;
                            }
                        }
                    }
                }
            }
            return true;
        }

        Matrix channelToMatrix(const Tensor4D &tensor, size_t batch_index, size_t channel_index) {
            size_t height = tensor.getHeight();
            size_t width = tensor.getWidth();
            Matrix result(height, width);

            size_t base_offset = (batch_index * tensor.getChannels() * height * width) +
                                 (channel_index * height * width);

            const float *tensor_data = tensor.getData().data() + base_offset;

            float *matrix_data = result.getData().data();

            size_t num_elements = height * width;

            size_t i = 0;
            for (; i + 7 < num_elements; i += 8) {
                __m256 tensor_values = _mm256_loadu_ps(tensor_data + i);
                _mm256_storeu_ps(matrix_data + i, tensor_values);
            }

            for (; i < num_elements; ++i) {
                matrix_data[i] = tensor_data[i];
            }

            return result;
        }


        [[nodiscard]] Tensor4D
        pad(const std::vector<std::pair<size_t, size_t>> &padding, float constant_value = 0.0f) const {
            if (padding.size() != 4) {
                throw std::invalid_argument("Padding should be specified for all 4 dimensions");
            }

            size_t new_batch = batch_size + padding[0].first + padding[0].second;
            size_t new_channels = channels + padding[1].first + padding[1].second;
            size_t new_height = height + padding[2].first + padding[2].second;
            size_t new_width = width + padding[3].first + padding[3].second;

            Tensor4D padded(new_batch, new_channels, new_height, new_width);
            padded.fill(constant_value);

            for (size_t n = 0; n < batch_size; ++n) {
                for (size_t c = 0; c < channels; ++c) {
                    for (size_t h = 0; h < height; ++h) {
                        for (size_t w = 0; w < width; ++w) {
                            padded(n + padding[0].first,
                                   c + padding[1].first,
                                   h + padding[2].first,
                                   w + padding[3].first) = (*this)(n, c, h, w);
                        }
                    }
                }
            }

            return padded;
        }

        const float &operator()(size_t i, size_t j) const {
            return this->operator()(0, i, j, 0);
        }

        float &operator()(size_t i, size_t j) {
            return this->operator()(0, i, j, 0);
        }


        size_t getBatchSize() const { return batch_size; }

        size_t getChannels() const { return channels; }

        size_t getHeight() const { return height; }

        size_t getWidth() const { return width; }

        const std::vector<float> &getData() const { return data; }

        std::vector<float> &getData() { return data; }

        Tensor4D(int batch_size, int channels, int height, int width, const std::vector<float> &data)
                : batch_size(batch_size), channels(channels), height(height), width(width), data(data) {}
    };

    Tensor4D create_tensor(const std::vector<std::vector<std::vector<std::vector<float>>>> &values);


} // namespace nnm


#include <vector>
#include <cmath>
#include <memory>
#include <optional>
#include <stdexcept>

namespace nnm {

    class BatchNorm2d : public Layer<Tensor4D, Tensor4D> {
    private:
        size_t num_features;
        double eps;
        std::optional<double> momentum;

        std::vector<float> weight;
        std::vector<float> bias;
        std::vector<float> running_mean;
        std::vector<float> running_var;

    public:
        BatchNorm2d(size_t num_features, double eps = 1e-5, std::optional<double> momentum = 0.1,
                    bool affine = true, bool track_running_stats = true)
                : num_features(num_features), eps(eps), momentum(momentum) {

            if (affine) {
                weight.resize(num_features, 1.0f);
                bias.resize(num_features, 0.0f);
            }

            if (track_running_stats) {
                running_mean.resize(num_features, 0.0f);
                running_var.resize(num_features, 1.0f);
            }
        }

        Tensor4D forward(const Tensor4D &input) override {
            if (input.getChannels() != num_features) {
                throw std::invalid_argument("Input channel dimension doesn't match num_features");
            }

            Tensor4D output(input.getBatchSize(), num_features, input.getHeight(), input.getWidth());

            for (size_t c = 0; c < num_features; ++c) {
                double mean = running_mean[c];
                double var = running_var[c];
                double gamma = weight[c];
                double beta = bias[c];
                double inv_std = 1.0 / std::sqrt(var + eps);

                // Première passe : calculer la moyenne et la variance exactes
                double sum = 0.0;
                double sq_sum = 0.0;
                size_t count = input.getBatchSize() * input.getHeight() * input.getWidth();

                for (size_t n = 0; n < input.getBatchSize(); ++n) {
                    for (size_t h = 0; h < input.getHeight(); ++h) {
                        for (size_t w = 0; w < input.getWidth(); ++w) {
                            double x = input(n, c, h, w);
                            sum += x;
                            sq_sum += x * x;
                        }
                    }
                }

                double batch_mean = sum / count;
                double batch_var = (sq_sum / count) - (batch_mean * batch_mean);

                // Deuxième passe : normaliser et appliquer gamma et beta
                for (size_t n = 0; n < input.getBatchSize(); ++n) {
                    for (size_t h = 0; h < input.getHeight(); ++h) {
                        for (size_t w = 0; w < input.getWidth(); ++w) {
                            double x = input(n, c, h, w);
                            double normalized = (x - batch_mean) / std::sqrt(batch_var + eps);
                            output(n, c, h, w) = static_cast<float>(gamma * normalized + beta);
                        }
                    }
                }
            }

            return output;
        }


        std::string get_name() const override {
            return "BatchNorm2d";
        }

        size_t get_input_size() const override {
            return num_features;
        }

        size_t get_output_size() const override {
            return num_features;
        }

        void set_parameters(const Tensor4D &weight, const Tensor4D &bias,
                            const Tensor4D &running_mean, const Tensor4D &running_var) {
            if (weight.getChannels() != num_features || bias.getChannels() != num_features ||
                running_mean.getChannels() != num_features || running_var.getChannels() != num_features) {
                throw std::invalid_argument("Parameter sizes do not match num_features");
            }

            for (size_t c = 0; c < num_features; ++c) {
                this->weight[c] = weight(0, c, 0, 0);
                this->bias[c] = bias(0, c, 0, 0);
                this->running_mean[c] = running_mean(0, c, 0, 0);
                this->running_var[c] = running_var(0, c, 0, 0);
            }
        }
    };

} // namespace nnm
/*-- #include "ConvolutionalLayer.h" start --*/

/*-- #include "Layer.h" start --*/
/*-- #include "Tensor4D.h" start --*/
/*-- #include "Matrix.h" start --*/
#include <random>

namespace nnm {

    class ConvolutionalLayer : public Layer<Tensor4D, Tensor4D> {
    private:
        size_t in_channels, out_channels, kernel_size, stride, padding;
        Tensor4D weights;
        Tensor4D bias;
        Tensor4D weight_gradients;
        Tensor4D bias_gradients;

    public:
        ConvolutionalLayer(size_t in_channels, size_t out_channels, size_t kernel_size,
                           size_t stride = 1, size_t padding = 0)
                : in_channels(in_channels), out_channels(out_channels), kernel_size(kernel_size),
                  stride(stride), padding(padding),
                  weights(out_channels, in_channels, kernel_size, kernel_size),
                  bias(1, out_channels, 1, 1),
                  weight_gradients(out_channels, in_channels, kernel_size, kernel_size),
                  bias_gradients(1, out_channels, 1, 1) {

            // Xavier/Glorot initialization for weights
            std::random_device rd;
            std::mt19937 gen(rd());
            float limit = std::sqrt(
                    6.0f / (in_channels * kernel_size * kernel_size + out_channels * kernel_size * kernel_size));
            std::uniform_real_distribution<> dis(-limit, limit);

            for (size_t o = 0; o < out_channels; ++o) {
                for (size_t i = 0; i < in_channels; ++i) {
                    for (size_t h = 0; h < kernel_size; ++h) {
                        for (size_t w = 0; w < kernel_size; ++w) {
                            weights(o, i, h, w) = static_cast<float>(dis(gen));
                        }
                    }
                }
            }

            // Initialize biases to zero
            bias.fill(0.0f);

            // Initialize gradients to zero
            weight_gradients.fill(0.0f);
            bias_gradients.fill(0.0f);
        }

        Tensor4D forward(const Tensor4D &input) override {
            size_t N = input.getBatchSize();
            size_t H = input.getHeight();
            size_t W = input.getWidth();

            size_t H_out = 1 + (H + 2 * padding - kernel_size) / stride;
            size_t W_out = 1 + (W + 2 * padding - kernel_size) / stride;

            Tensor4D output(N, out_channels, H_out, W_out);

            Tensor4D padded_input = input.pad({{0,       0},
                                               {0,       0},
                                               {padding, padding},
                                               {padding, padding}});

            for (size_t n = 0; n < N; ++n) {
                for (size_t f = 0; f < out_channels; ++f) {
                    int height_index = 0;
                    for (size_t i = 0; i < H; i += stride) {
                        int width_index = 0;
                        for (size_t j = 0; j < W; j += stride) {
                            Tensor4D x_slice = padded_input.subTensor(n, 0, i, j,
                                                                      1, in_channels, kernel_size, kernel_size);
                            Tensor4D w_slice = weights.subTensor(f, 0, 0, 0,
                                                                 1, in_channels, kernel_size, kernel_size);
                            float sum = x_slice.elementWiseMul(w_slice).sum() + bias(0, f, 0, 0);
                            output(n, f, height_index, width_index) = sum;
                            width_index++;
                        }
                        height_index++;
                    }
                }
            }

            return output;
        }

        [[nodiscard]] std::string get_name() const override {
            return "ConvolutionalLayer";
        }

        [[nodiscard]] size_t get_input_size() const override {
            return in_channels;
        }

        [[nodiscard]] size_t get_output_size() const override {
            return out_channels;
        }

        void set_weights(const Tensor4D &new_weights) {
            weights = new_weights;
        }

        void set_bias(const Tensor4D &new_bias) {
            bias = new_bias;
        }

        [[nodiscard]] const Tensor4D &get_weights() const { return weights; }

        [[nodiscard]] const Tensor4D &get_bias() const { return bias; }

        [[nodiscard]] size_t get_padding() const { return padding; }

        [[nodiscard]] size_t get_kernel_size() const { return kernel_size; }

        [[nodiscard]] size_t get_stride() const { return stride; }

        Tensor4D get_weight_gradients() {
            return weight_gradients;
        }

        Tensor4D get_bias_gradients() {
            return bias_gradients;
        }
    };

} // namespace nnm
/*-- #include "FlattenLayer.h" start --*/

/*-- #include "Layer.h" start --*/
/*-- #include "Tensor4D.h" start --*/
#include <stdexcept>

namespace nnm {

    class Flatten : public Layer<Tensor4D, Tensor4D> {
    private:
        int start_dim;
        int end_dim;

    public:
        Flatten(int start_dim = 1, int end_dim = -1) : start_dim(start_dim), end_dim(end_dim) {}

        Tensor4D forward(const Tensor4D &input) override {
            size_t batch_size = input.getBatchSize();
            size_t channels = input.getChannels();
            size_t height = input.getHeight();
            size_t width = input.getWidth();

            int real_end_dim = (end_dim == -1) ? 3 : end_dim;

            if (start_dim < 0 || start_dim > 3 || real_end_dim < 0 || real_end_dim > 3 || start_dim > real_end_dim) {
                throw std::invalid_argument("Invalid start_dim or end_dim");
            }

            size_t rows = batch_size;
            size_t cols = channels * height * width;

            Tensor4D output(1, rows, cols, 1);

            for (size_t n = 0; n < batch_size; ++n) {
                size_t index = 0;
                for (size_t c = 0; c < channels; ++c) {
                    for (size_t h = 0; h < height; ++h) {
                        for (size_t w = 0; w < width; ++w) {
                            output(0, n, index++, 0) = input(n, c, h, w);
                        }
                    }
                }
            }

            return output;
        }

        std::string get_name() const override {
            return "Flatten";
        }

        size_t get_input_size() const override {
            return 0;  // Not applicable for Flatten
        }

        size_t get_output_size() const override {
            return 0;  // Not applicable for Flatten
        }

    };

} // namespace nnm
/*-- File: Foo.cpp start --*/

/*-- File: Foo.cpp end --*/
/*-- #include "Layer.h" start --*/
/*-- #include "LinearLayer.h" start --*/

/*-- #include "Layer.h" start --*/
/*-- #include "Tensor4D.h" start --*/
#include <random>
#include <cmath>
#include <stdexcept>
#include <iostream>

namespace nnm {

    class LinearLayer : public Layer<Tensor4D, Tensor4D> {
    private:
        Tensor4D weights;
        Tensor4D bias;
        size_t in_features;
        size_t out_features;

    public:
        LinearLayer(size_t in_features, size_t out_features)
                : in_features(in_features), out_features(out_features),
                  weights(1, out_features, in_features, 1),
                  bias(1, out_features, 1, 1) {

            // Xavier/Glorot initialization
            std::random_device rd;
            std::mt19937 gen(rd());
            std::uniform_real_distribution<> dis(-1.0 / std::sqrt(in_features), 1.0 / std::sqrt(in_features));

            for (size_t i = 0; i < out_features; ++i) {
                for (size_t j = 0; j < in_features; ++j) {
                    weights(0, i, j, 0) = dis(gen);
                }
                bias(0, i, 0, 0) = 0.0f;
            }
        }

        Tensor4D forward(const Tensor4D &input) override {
            size_t batch_size = input.getBatchSize();
            size_t input_size = input.getChannels() * input.getHeight() * input.getWidth();

            if (input_size != in_features) {
                throw std::invalid_argument("Input size does not match layer's in_features");
            }

            std::cout << "Input dimensions: " << input.getBatchSize() << "x" << input.getChannels()
                      << "x" << input.getHeight() << "x" << input.getWidth() << std::endl;
            std::cout << "Weight dimensions: " << weights.getBatchSize() << "x" << weights.getChannels()
                      << "x" << weights.getHeight() << "x" << weights.getWidth() << std::endl;

            Tensor4D output(batch_size, out_features, 1, 1);

            for (size_t n = 0; n < batch_size; ++n) {
                for (size_t j = 0; j < out_features; ++j) {
                    float result = 0.0f;
                    for (size_t i = 0; i < in_features; ++i) {
                        size_t c = i / (input.getHeight() * input.getWidth());
                        size_t h = (i % (input.getHeight() * input.getWidth())) / input.getWidth();
                        size_t w = (i % (input.getHeight() * input.getWidth())) % input.getWidth();
                        result += input(n, c, h, w) * weights(0, j, i, 0);
                    }
                    output(n, j, 0, 0) = result + bias(0, j, 0, 0);
                }
            }

            return output;
        }

        std::string get_name() const override {
            return "LinearLayer";
        }

        size_t get_input_size() const override {
            return in_features;
        }

        size_t get_output_size() const override {
            return out_features;
        }

        const Tensor4D &get_weights() const {
            return weights;
        }

        const Tensor4D &get_bias() const {
            return bias;
        }

        void set_weights(const Tensor4D &new_weights) {
            if (new_weights.getBatchSize() != 1 ||
                new_weights.getChannels() != out_features ||
                new_weights.getHeight() != in_features ||
                new_weights.getWidth() != 1) {
                throw std::invalid_argument("New weights dimensions do not match layer dimensions");
            }
            weights = new_weights;
        }

        void set_bias(const Tensor4D &new_bias) {
            if (new_bias.getBatchSize() != 1 ||
                new_bias.getChannels() != out_features ||
                new_bias.getHeight() != 1 ||
                new_bias.getWidth() != 1) {
                throw std::invalid_argument("New bias dimensions do not match layer dimensions");
            }
            bias = new_bias;
        }
    };

} // namespace nnm
/*-- #include "LossFunctions.h" start --*/

/*-- #include "Tensor4D.h" start --*/
#include <cmath>
#include <limits>
#include <stdexcept>

namespace nnm {

    class LossFunctions {
    public:
        struct SoftmaxLossResult {
            float loss;
            Tensor4D gradient;
        };

        static SoftmaxLossResult softmax_loss(const Tensor4D &x, const Tensor4D &y) {
            if (x.getBatchSize() != y.getBatchSize() || y.getChannels() != 1 || y.getHeight() != 1 ||
                y.getWidth() != 1) {
                throw std::invalid_argument("Dimensions of x and y must match, and y should be a 1D tensor");
            }

            size_t N = x.getBatchSize();
            size_t C = x.getChannels();

            // Calculate shifted logits
            Tensor4D shifted_logits(N, C, 1, 1);
            for (size_t i = 0; i < N; ++i) {
                float max_val = -std::numeric_limits<float>::max();
                for (size_t j = 0; j < C; ++j) {
                    max_val = std::max(max_val, x(i, j, 0, 0));
                }
                for (size_t j = 0; j < C; ++j) {
                    shifted_logits(i, j, 0, 0) = x(i, j, 0, 0) - max_val;
                }
            }

            // Calculate sum of exp(shifted_logits)
            Tensor4D z(N, 1, 1, 1);
            for (size_t i = 0; i < N; ++i) {
                float sum = 0.0f;
                for (size_t j = 0; j < C; ++j) {
                    sum += std::exp(shifted_logits(i, j, 0, 0));
                }
                z(i, 0, 0, 0) = sum;
            }

            // Calculate log probabilities and probabilities
            Tensor4D log_probs(N, C, 1, 1);
            Tensor4D probs(N, C, 1, 1);
            for (size_t i = 0; i < N; ++i) {
                for (size_t j = 0; j < C; ++j) {
                    log_probs(i, j, 0, 0) = shifted_logits(i, j, 0, 0) - std::log(z(i, 0, 0, 0));
                    probs(i, j, 0, 0) = std::exp(log_probs(i, j, 0, 0));
                }
            }

            // Calculate loss
            float loss = 0.0f;
            for (size_t i = 0; i < N; ++i) {
                size_t label = static_cast<size_t>(y(i, 0, 0, 0));
                if (label < 0 || label >= C) {
                    throw std::out_of_range("Label must be between 0 and C-1");
                }
                loss -= log_probs(i, label, 0, 0);
            }
            loss /= N;

            // Calculate gradient
            Tensor4D dx = probs;
            for (size_t i = 0; i < N; ++i) {
                size_t label = static_cast<size_t>(y(i, 0, 0, 0));
                dx(i, label, 0, 0) -= 1.0f;
            }
            for (size_t i = 0; i < N; ++i) {
                for (size_t j = 0; j < C; ++j) {
                    dx(i, j, 0, 0) /= N;
                }
            }

            return {loss, dx};
        }
    };

} // namespace nnm
/*-- #include "Matrix.h" start --*/
/*-- #include "MaxPoolingLayer.h" start --*/

/*-- #include "Layer.h" start --*/
/*-- #include "Tensor4D.h" start --*/
#include <cmath>
#include <algorithm>

namespace nnm {

    class MaxPoolingLayer : public Layer<Tensor4D, Tensor4D> {
    private:
        size_t pooling_height;
        size_t pooling_width;
        size_t stride;

    public:
        MaxPoolingLayer(size_t pooling_height, size_t pooling_width, size_t stride)
                : pooling_height(pooling_height), pooling_width(pooling_width), stride(stride) {}

        Tensor4D forward(const Tensor4D &x) override {
            size_t N = x.getBatchSize();
            size_t F = x.getChannels();
            size_t H = x.getHeight();
            size_t W = x.getWidth();

            size_t height_pooled_out = 1 + (H - pooling_height) / stride;
            size_t width_pooled_out = 1 + (W - pooling_width) / stride;

            Tensor4D pooled_output(N, F, height_pooled_out, width_pooled_out);

            for (size_t n = 0; n < N; ++n) {
                for (size_t f = 0; f < F; ++f) {
                    for (size_t i = 0; i < height_pooled_out; ++i) {
                        for (size_t j = 0; j < width_pooled_out; ++j) {
                            size_t ii = i * stride;
                            size_t jj = j * stride;

                            float max_val = std::numeric_limits<float>::lowest();
                            for (size_t ph = 0; ph < pooling_height; ++ph) {
                                for (size_t pw = 0; pw < pooling_width; ++pw) {
                                    max_val = std::max(max_val, x(n, f, ii + ph, jj + pw));
                                }
                            }
                            pooled_output(n, f, i, j) = max_val;
                        }
                    }
                }
            }

            return pooled_output;
        }

        std::string get_name() const override {
            return "MaxPoolingLayer";
        }

        size_t get_input_size() const override {
            // This should return the input volume size, but it's not applicable for pooling layers
            return 0;
        }

        size_t get_output_size() const override {
            // This should return the output volume size, but it's not applicable for pooling layers
            return 0;
        }

    };

} // namespace nnm
/*-- #include "ReLULayer.h" start --*/

/*-- #include "Layer.h" start --*/
/*-- #include "Tensor4D.h" start --*/
#include <algorithm>
#include <immintrin.h>

namespace nnm {

    class ReLULayer : public Layer<Tensor4D, Tensor4D> {
    public:
        ReLULayer() = default;

        Tensor4D forward(const Tensor4D &x) override {
            Tensor4D relu_output(x.getBatchSize(), x.getChannels(), x.getHeight(), x.getWidth());

            const float *input_data = x.getData().data();
            float *output_data = relu_output.getData().data();
            size_t total_elements = x.getBatchSize() * x.getChannels() * x.getHeight() * x.getWidth();

            // SIMD optimization for AVX2
            size_t i = 0;
            for (; i + 7 < total_elements; i += 8) {
                __m256 input_vec = _mm256_loadu_ps(input_data + i);
                __m256 zero_vec = _mm256_setzero_ps();
                __m256 result_vec = _mm256_max_ps(input_vec, zero_vec);
                _mm256_storeu_ps(output_data + i, result_vec);
            }

            // Handle remaining elements
            for (; i < total_elements; ++i) {
                output_data[i] = std::max(0.0f, input_data[i]);
            }

            return relu_output;
        }

        std::string get_name() const override {
            return "ReLULayer";
        }

        size_t get_input_size() const override {
            return 0;  // ReLU doesn't change the size
        }

        size_t get_output_size() const override {
            return 0;  // ReLU doesn't change the size
        }

    };

} // namespace nnm
/*-- #include "ResBlock.h" start --*/

#include <memory>
/*-- #include "Tensor4D.h" start --*/
/*-- #include "Sequential.h" start --*/

#include <vector>
#include <memory>
#include <stdexcept>
#include <string>
/*-- #include "Layer.h" start --*/
/*-- #include "Tensor4D.h" start --*/

namespace nnm {

    class Sequential : public Layer<Tensor4D, Tensor4D> {
    private:
        std::vector<std::unique_ptr<Layer<Tensor4D, Tensor4D>>> layers;

    public:
        Sequential() = default;

        // Add a layer to the sequence
        void add_layer(std::unique_ptr<Layer<Tensor4D, Tensor4D>> layer) {
            if (layers.empty()) {
                layers.push_back(std::move(layer));
            } else {
                // Ensure the layer's input size matches the previous layer's output size
                size_t prev_output_size = layers.back()->get_output_size();
                if (prev_output_size != layer->get_input_size() &&
                    prev_output_size != 0 && layer->get_input_size() != 0) {
                    throw std::invalid_argument("Layer input size does not match the previous layer's output size.");
                }
                layers.push_back(std::move(layer));
            }
        }

        Tensor4D forward(const Tensor4D &input) override {
            Tensor4D output = input;
            for (const auto &layer: layers) {
                output = layer->forward(output);
            }
            return output;
        }

        std::string get_name() const override {
            return "Sequential";
        }

        size_t get_input_size() const override {
            if (layers.empty()) {
                throw std::runtime_error("Sequential model is empty.");
            }
            return layers.front()->get_input_size();
        }

        size_t get_output_size() const override {
            if (layers.empty()) {
                throw std::runtime_error("Sequential model is empty.");
            }
            return layers.back()->get_output_size();
        }


        std::string extra_repr() const {
            std::string repr = "Layers:\n";
            for (const auto &layer: layers) {
                repr += "  " + layer->get_name() + "\n";
            }
            return repr;
        }
    };

} // namespace nnm

/*-- #include "ConvolutionalLayer.h" start --*/
/*-- #include "BatchNorm2d.h" start --*/
/*-- #include "ReLULayer.h" start --*/

namespace nnm {
    class ResBlock : public Layer<Tensor4D, Tensor4D> {
    private:
        std::unique_ptr<ConvolutionalLayer> conv1;
        std::unique_ptr<BatchNorm2d> bn1;
        std::unique_ptr<ConvolutionalLayer> conv2;
        std::unique_ptr<BatchNorm2d> bn2;
        std::unique_ptr<ReLULayer> relu;

    public:
        ResBlock(size_t num_hidden) {
            conv1 = std::make_unique<ConvolutionalLayer>(num_hidden, num_hidden, 3, 1, 1);
            bn1 = std::make_unique<BatchNorm2d>(num_hidden);
            conv2 = std::make_unique<ConvolutionalLayer>(num_hidden, num_hidden, 3, 1, 1);
            bn2 = std::make_unique<BatchNorm2d>(num_hidden);
            relu = std::make_unique<ReLULayer>();
        }

        Tensor4D forward(const Tensor4D &input) override {
            Tensor4D x = input;
            Tensor4D residual = x;
            x = relu->forward(bn1->forward(conv1->forward(x)));
            x = bn2->forward(conv2->forward(x));
            x = x + residual;  // Assuming Tensor4D supports element-wise addition
            x = relu->forward(x);
            return x;
        }

        std::string get_name() const override { return "ResBlock"; }

        size_t get_input_size() const override {
            return conv1->get_input_size();
        }

        size_t get_output_size() const override {
            return conv2->get_output_size();
        }

    };
}
/*-- #include "ResNet.h" start --*/

#include <memory>
/*-- #include "Tensor4D.h" start --*/
/*-- #include "Sequential.h" start --*/
/*-- #include "ResBlock.h" start --*/
/*-- #include "FlattenLayer.h" start --*/
/*-- #include "LinearLayer.h" start --*/
/*-- #include "Tanh.h" start --*/

/*-- #include "Layer.h" start --*/
/*-- #include "Tensor4D.h" start --*/
#include <cmath>
#include <limits>

namespace nnm {

    class Tanh : public Layer<Tensor4D, Tensor4D> {
    private:
        static constexpr double tiny = 1.0e-300;
        static constexpr double huge = 1.0e300;

        static double tanh_impl(double x) {
            double t, z;
            int32_t jx, ix;

            union {
                double f;
                uint64_t i;
            } u = {x};
            jx = u.i >> 32;
            ix = jx & 0x7fffffff;

            if (ix >= 0x7ff00000) {
                if (jx >= 0)
                    return 1.0 / x + 1.0;  // tanh(+-inf)=+-1
                else
                    return 1.0 / x - 1.0;  // tanh(NaN) = NaN
            }

            if (ix < 0x40360000) {  // |x| < 22
                if (ix < 0x3e300000) {  // |x| < 2**-28
                    if (huge + x > 1.0)
                        return x;
                }
                if (ix >= 0x3ff00000) {  // |x| >= 1
                    t = std::expm1(2.0 * std::fabs(x));
                    z = 1.0 - 2.0 / (t + 2.0);
                } else {
                    t = std::expm1(-2.0 * std::fabs(x));
                    z = -t / (t + 2.0);
                }
            } else {  // |x| >= 22, return +-1
                z = 1.0 - tiny;
            }
            return jx >= 0 ? z : -z;
        }

    public:
        Tanh() = default;

        Tensor4D forward(const Tensor4D &input) override {
            Tensor4D output(input.getBatchSize(), input.getChannels(), input.getHeight(), input.getWidth());

            for (size_t n = 0; n < input.getBatchSize(); ++n) {
                for (size_t c = 0; c < input.getChannels(); ++c) {
                    for (size_t h = 0; h < input.getHeight(); ++h) {
                        for (size_t w = 0; w < input.getWidth(); ++w) {
                            output(n, c, h, w) = static_cast<float>(tanh_impl(input(n, c, h, w)));
                        }
                    }
                }
            }

            return output;
        }

        std::string get_name() const override {
            return "Tanh";
        }

        size_t get_input_size() const override {
            return 0;  // Not applicable for Tanh
        }

        size_t get_output_size() const override {
            return 0;  // Not applicable for Tanh
        }

    };

} // namespace nnm

namespace nnm {

    class ResNet : public Layer<Tensor4D, std::pair<Tensor4D, Tensor4D>> {
    private:
        std::unique_ptr<Sequential> startBlock;
        std::vector<std::unique_ptr<ResBlock>> backBone;
        std::unique_ptr<Sequential> policyHead;
        std::unique_ptr<Sequential> valueHead;

        size_t action_size;
        size_t row_count;
        size_t column_count;

    public:
        ResNet(size_t num_resBlocks, size_t num_hidden, size_t action_size, size_t row_count, size_t column_count)
                : action_size(action_size), row_count(row_count), column_count(column_count) {

            startBlock = std::make_unique<Sequential>();
            startBlock->add_layer(std::make_unique<ConvolutionalLayer>(3, num_hidden, 3, 1, 1));
            startBlock->add_layer(std::make_unique<BatchNorm2d>(num_hidden));
            startBlock->add_layer(std::make_unique<ReLULayer>());

            for (size_t i = 0; i < num_resBlocks; ++i) {
                backBone.push_back(std::make_unique<ResBlock>(num_hidden));
            }

            policyHead = std::make_unique<Sequential>();
            policyHead->add_layer(std::make_unique<ConvolutionalLayer>(num_hidden, 32, 3, 1, 1));
            policyHead->add_layer(std::make_unique<BatchNorm2d>(32));
            policyHead->add_layer(std::make_unique<ReLULayer>());
            policyHead->add_layer(std::make_unique<Flatten>());
            policyHead->add_layer(std::make_unique<LinearLayer>(32 * row_count * column_count, action_size));

            valueHead = std::make_unique<Sequential>();
            valueHead->add_layer(std::make_unique<ConvolutionalLayer>(num_hidden, 3, 3, 1, 1));
            valueHead->add_layer(std::make_unique<BatchNorm2d>(3));
            valueHead->add_layer(std::make_unique<ReLULayer>());
            valueHead->add_layer(std::make_unique<Flatten>());
            valueHead->add_layer(std::make_unique<LinearLayer>(3 * row_count * column_count, 1));
            valueHead->add_layer(std::make_unique<Tanh>());
        }

        std::pair<Tensor4D, Tensor4D> forward(const Tensor4D &input) override {
            Tensor4D x = startBlock->forward(input);

            for (const auto &resBlock: backBone) {
                x = resBlock->forward(x);
            }

            Tensor4D policy = policyHead->forward(x);

            Tensor4D value = valueHead->forward(x);
            return {policy, value};
        }

        std::string get_name() const override {
            return "ResNet";
        }

        size_t get_input_size() const override {
            return row_count * column_count; // Adjust based on actual input size requirements
        }

        size_t get_output_size() const override {
            return action_size; // Adjust based on actual output size requirements
        }
    };
}

/*-- #include "Sequential.h" start --*/
/*-- #include "Tanh.h" start --*/
/*-- #include "Tensor4D.h" start --*/
/*-- #include "TicTacToeModel.h" start --*/
#pragma  once

/*-- #include "ConvolutionalLayer.h" start --*/
/*-- #include "BatchNorm2d.h" start --*/
/*-- #include "MaxPoolingLayer.h" start --*/
/*-- #include "FlattenLayer.h" start --*/
/*-- #include "LinearLayer.h" start --*/
/*-- #include "ReLULayer.h" start --*/
/*-- #include "Tanh.h" start --*/
/*-- #include "Tensor4D.h" start --*/
#include <vector>

namespace nnm {

    class TicTacToeModel {
    private:
        ConvolutionalLayer conv1;
        BatchNorm2d bn1;
        MaxPoolingLayer pool;
        Flatten flatten;
        LinearLayer fc1;
        LinearLayer fc2;
        LinearLayer fc3;

    public:
        TicTacToeModel() :
                conv1(3, 16, 3, 1, 1),
                bn1(16),
                pool(2, 2, 2),
                flatten(),
                fc1(16 * 1 * 1, 32),
                fc2(32, 10),
                fc3(32, 1) {
            // Initialize weights and biases
            std::vector<float> conv1_weights = {0.147677f, 0.032783f, 0.035251f, -0.019098f, -0.040287f, 0.004235f,
                                                0.013149f, 0.005294f, -0.070481f, 0.076407f, -0.089488f, 0.130583f,
                                                0.050023f, -0.186164f, 0.107362f, -0.047645f, -0.121480f, 0.152359f,
                                                0.159149f, -0.169009f, -0.151522f, 0.135883f, -0.044136f, -0.115255f,
                                                0.128292f, 0.120739f, -0.065597f, 0.185649f, 0.068083f, -0.135633f,
                                                -0.072527f, -0.137348f, 0.142532f, -0.148532f, -0.158227f, 0.061697f,
                                                -0.003450f, -0.101532f, -0.014827f, -0.057316f, 0.097058f, -0.108733f,
                                                -0.170381f, 0.077483f, -0.112282f, 0.121498f, 0.074242f, 0.048651f,
                                                -0.121164f, -0.014811f, 0.094744f, -0.183474f, 0.174077f, 0.033692f,
                                                -0.081220f, -0.135098f, -0.043790f, 0.036683f, 0.008698f, -0.146825f,
                                                -0.162126f, -0.048057f, 0.101557f, 0.175989f, 0.005950f, -0.140973f,
                                                -0.052728f, -0.084429f, 0.072082f, -0.105465f, -0.063373f, -0.044388f,
                                                0.129507f, 0.004965f, -0.125152f, 0.019063f, 0.106037f, -0.055360f,
                                                0.043908f, 0.127529f, 0.157284f, -0.116364f, 0.047491f, 0.149055f,
                                                -0.001240f, -0.155459f, 0.084549f, 0.053515f, 0.161597f, 0.022283f,
                                                0.159949f, -0.089570f, -0.045829f, 0.102347f, -0.138221f, -0.085894f,
                                                0.134807f, -0.114323f, -0.020993f, -0.121405f, -0.158329f, 0.002874f,
                                                -0.174708f, 0.084287f, 0.030016f, 0.120374f, 0.134286f, -0.112170f,
                                                -0.127318f, -0.121583f, -0.031213f, 0.000520f, 0.065566f, -0.019956f,
                                                0.080602f, 0.116361f, -0.132831f, -0.039886f, -0.002458f, -0.005743f,
                                                -0.154382f, -0.149272f, -0.101225f, -0.060566f, -0.057839f, -0.089250f,
                                                0.168525f, -0.122481f, 0.120872f, 0.008069f, -0.121111f, -0.123023f,
                                                -0.137913f, 0.087317f, 0.102706f, -0.005752f, 0.021639f, 0.007680f,
                                                0.094929f, 0.053625f, 0.111155f, 0.059620f, -0.074808f, -0.063789f,
                                                0.016156f, -0.082751f, -0.136432f, -0.058046f, -0.108997f, -0.174791f,
                                                0.042377f, 0.111177f, -0.004017f, 0.132977f, 0.171581f, -0.069502f,
                                                -0.185534f, 0.002383f, 0.157207f, 0.170903f, 0.130454f, -0.010018f,
                                                -0.081413f, 0.124629f, -0.151670f, 0.093516f, 0.169781f, 0.089028f,
                                                -0.073902f, 0.125223f, 0.031969f, 0.002876f, -0.029631f, 0.079627f,
                                                -0.011510f, 0.067842f, 0.105940f, 0.049683f, 0.102083f, 0.046592f,
                                                -0.037020f, -0.114123f, -0.110729f, 0.045153f, -0.072836f, 0.037753f,
                                                -0.086386f, -0.114345f, -0.116084f, -0.163292f, -0.133285f, 0.020197f,
                                                0.002247f, 0.077144f, -0.026454f, 0.025189f, -0.085338f, -0.146202f,
                                                -0.024015f, -0.107179f, 0.115177f, 0.172978f, -0.051001f, 0.111736f,
                                                -0.105093f, -0.068383f, 0.182954f, -0.018952f, 0.161559f, 0.131235f,
                                                0.052319f, -0.010005f, 0.134186f, 0.155319f, 0.109188f, -0.146808f,
                                                -0.058432f, -0.158689f, 0.148046f, -0.149984f, -0.138520f, -0.126456f,
                                                -0.063722f, 0.069885f, 0.144774f, 0.169636f, 0.145236f, -0.077782f,
                                                0.126050f, 0.023486f, 0.172934f, 0.076181f, -0.022752f, 0.170850f,
                                                0.150332f, -0.072974f, 0.042779f, 0.094163f, 0.179701f, -0.039228f,
                                                -0.080466f, -0.081839f, 0.103974f, -0.089104f, 0.135120f, 0.099416f,
                                                -0.166566f, -0.131206f, 0.057312f, -0.090922f, -0.161928f, -0.050242f,
                                                -0.170162f, -0.005353f, 0.108635f, 0.062328f, -0.035770f, 0.177991f,
                                                0.156090f, -0.183947f, -0.155904f, -0.022254f, 0.153330f, -0.159994f,
                                                0.032715f, -0.066387f, -0.168348f, 0.019775f, 0.027524f, -0.028812f,
                                                -0.081000f, 0.014946f, -0.178317f, -0.099360f, -0.027996f, 0.079846f,
                                                -0.094348f, 0.181841f, 0.114739f, 0.069401f, 0.113407f, 0.008592f,
                                                0.007307f, 0.098610f, 0.018078f, -0.168438f, -0.007017f, -0.059907f,
                                                -0.094113f, -0.174628f, 0.110721f, 0.104032f, 0.039143f, 0.042960f,
                                                -0.080041f, -0.057938f, 0.027214f, 0.031773f, -0.142680f, 0.167030f,
                                                0.097607f, 0.042064f, 0.004608f, 0.136990f, -0.124368f, -0.016672f,
                                                -0.123740f, -0.093145f, 0.100810f, 0.081229f, 0.158989f, 0.044259f,
                                                -0.100319f, 0.002821f, -0.101956f, -0.036632f, 0.117077f, 0.177256f,
                                                -0.040688f, 0.037157f, -0.075883f, 0.056375f, -0.014810f, 0.071279f,
                                                -0.017624f, -0.136430f, 0.023121f, -0.040842f, 0.009103f, -0.127645f,
                                                0.130896f, 0.117693f, -0.060013f, 0.073324f, -0.105069f, 0.075354f,
                                                0.071027f, -0.029499f, -0.095549f, -0.105354f, -0.003160f, 0.166170f,
                                                -0.025519f, 0.166264f, 0.001387f, 0.079052f, 0.096020f, 0.140646f,
                                                -0.155086f, 0.006088f, 0.142048f, -0.177459f, -0.053690f, -0.104170f,
                                                0.087598f, -0.105770f, 0.111870f, -0.053322f, -0.181997f, 0.101360f,
                                                0.034846f, 0.134463f, 0.129316f, -0.120657f, 0.154493f, 0.114869f,
                                                -0.026237f, 0.066316f, -0.059980f, -0.054959f, -0.003103f, -0.116019f,
                                                0.034720f, 0.176253f, -0.081169f, 0.123938f, 0.072368f, -0.084685f,
                                                0.006047f, -0.066251f, -0.174109f, -0.097523f, -0.066181f, 0.142453f,
                                                -0.099867f, 0.097786f, -0.062031f, 0.121032f, 0.154738f, -0.125572f,
                                                -0.063897f, -0.078246f, 0.079037f, 0.013855f, 0.139386f, -0.144506f,
                                                0.098845f, 0.155394f, -0.144053f, 0.122361f, 0.144105f, -0.007478f,
                                                0.147593f, 0.171390f, 0.084158f, -0.061648f, -0.117880f, 0.008016f,
                                                -0.133777f, -0.078627f, 0.073383f, 0.010244f, 0.111063f, -0.118751f,
                                                0.019381f, -0.070260f, 0.016831f, 0.080079f, -0.010094f, -0.090431f,
                                                0.111509f, 0.004908f, 0.052780f, 0.177555f, -0.118162f, -0.152325f,
                                                0.036689f, 0.013596f, 0.002836f, -0.090390f, -0.026198f, -0.077129f};
            std::vector<float> conv1_bias = {0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f,
                                             0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f,
                                             0.000000f, 0.000000f, 0.000000f, 0.000000f};
            conv1.set_weights(Tensor4D(16, 3, 3, 3, conv1_weights));
            conv1.set_bias(Tensor4D(1, 16, 1, 1, conv1_bias));

            std::vector<float> bn1_weight = {1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f,
                                             1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f,
                                             1.000000f, 1.000000f, 1.000000f, 1.000000f};
            std::vector<float> bn1_bias = {0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f,
                                           0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f,
                                           0.000000f, 0.000000f};
            std::vector<float> bn1_running_mean = {0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f,
                                                   0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f,
                                                   0.000000f, 0.000000f, 0.000000f, 0.000000f};
            std::vector<float> bn1_running_var = {1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f,
                                                  1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f,
                                                  1.000000f, 1.000000f, 1.000000f, 1.000000f};
            bn1.set_parameters(
                    Tensor4D(1, 16, 1, 1, bn1_weight),
                    Tensor4D(1, 16, 1, 1, bn1_bias),
                    Tensor4D(1, 16, 1, 1, bn1_running_mean),
                    Tensor4D(1, 16, 1, 1, bn1_running_var)
            );

            std::vector<float> fc1_weights = {0.085397f, -0.165659f, -0.021534f, -0.319643f, -0.271843f, -0.315379f,
                                              0.186913f, 0.282141f, -0.096514f, -0.259889f, -0.068255f, -0.221802f,
                                              0.273785f, -0.288466f, -0.208919f, 0.316905f, -0.240765f, 0.016048f,
                                              -0.234909f, 0.285462f, 0.114718f, 0.222632f, 0.116819f, 0.233920f,
                                              0.295250f, 0.180988f, 0.015245f, -0.210197f, 0.213042f, -0.230028f,
                                              -0.119261f, 0.198970f, -0.015572f, -0.188572f, 0.238388f, 0.255884f,
                                              -0.157863f, 0.029331f, -0.062577f, 0.065461f, -0.278664f, 0.232287f,
                                              -0.034084f, -0.209460f, -0.170996f, -0.315742f, 0.241338f, 0.156549f,
                                              -0.140352f, -0.216810f, 0.119757f, 0.337961f, -0.217969f, -0.209074f,
                                              -0.314399f, -0.276730f, -0.171250f, -0.322853f, -0.152517f, -0.087376f,
                                              -0.265064f, -0.339237f, -0.146932f, -0.291552f, -0.036222f, 0.341376f,
                                              0.136835f, -0.094188f, -0.339266f, 0.008747f, 0.029544f, 0.281563f,
                                              -0.031122f, -0.246775f, 0.167454f, -0.214300f, -0.267581f, -0.110933f,
                                              0.323464f, -0.298794f, -0.246145f, 0.307553f, -0.320302f, -0.100232f,
                                              -0.063347f, 0.334043f, 0.304773f, -0.096660f, 0.115175f, -0.188392f,
                                              -0.127392f, 0.014632f, -0.119701f, 0.066074f, 0.110645f, 0.195058f,
                                              -0.053284f, 0.142602f, -0.310325f, 0.337073f, 0.244154f, 0.114208f,
                                              0.092814f, -0.324246f, -0.196966f, -0.283090f, 0.083833f, -0.144916f,
                                              -0.058488f, -0.113732f, -0.061623f, -0.341673f, 0.309987f, -0.230871f,
                                              0.186482f, 0.295939f, 0.049781f, -0.344690f, 0.226002f, 0.052642f,
                                              0.011504f, 0.306154f, 0.289010f, -0.109763f, -0.128122f, -0.191233f,
                                              -0.155641f, -0.267652f, 0.339664f, 0.030931f, -0.288521f, 0.086761f,
                                              -0.141886f, 0.228467f, -0.305348f, 0.272636f, -0.004236f, -0.316484f,
                                              0.179786f, -0.281129f, -0.253375f, -0.305602f, -0.278569f, -0.283714f,
                                              -0.182261f, 0.334867f, -0.077822f, -0.216880f, -0.321637f, 0.037062f,
                                              0.222788f, -0.048233f, -0.161970f, 0.310859f, 0.330603f, -0.077571f,
                                              -0.224952f, -0.219577f, 0.204920f, 0.113547f, 0.284229f, -0.287882f,
                                              0.260531f, -0.185786f, -0.299287f, 0.145043f, 0.316378f, -0.053185f,
                                              0.128723f, -0.012790f, -0.004993f, 0.091534f, 0.156284f, -0.215446f,
                                              0.038619f, -0.052773f, -0.123721f, -0.279976f, 0.087627f, -0.242149f,
                                              -0.339657f, 0.235994f, -0.232856f, -0.137108f, 0.195772f, 0.058856f,
                                              -0.277312f, 0.069299f, 0.323558f, -0.341594f, -0.332326f, -0.077399f,
                                              0.118870f, 0.202664f, -0.200478f, -0.029645f, -0.218270f, 0.145540f,
                                              -0.203265f, 0.033279f, 0.023368f, 0.041526f, 0.110397f, -0.012771f,
                                              0.070584f, 0.113190f, 0.011353f, -0.003042f, -0.123370f, -0.254832f,
                                              0.335085f, 0.132880f, -0.214477f, -0.258278f, 0.338752f, -0.259885f,
                                              -0.116043f, -0.183099f, 0.252956f, 0.126334f, 0.322196f, -0.335419f,
                                              0.178925f, 0.338775f, 0.246808f, -0.111216f, -0.235894f, 0.027129f,
                                              0.303606f, 0.156945f, -0.076225f, -0.232926f, 0.199407f, -0.171983f,
                                              -0.031049f, 0.037269f, -0.061487f, -0.127078f, 0.198327f, 0.109876f,
                                              -0.189979f, 0.226716f, 0.331199f, -0.334040f, -0.263747f, -0.319745f,
                                              -0.240931f, -0.225273f, -0.335737f, -0.312425f, -0.003568f, 0.196197f,
                                              -0.091049f, 0.120071f, 0.279287f, 0.080715f, 0.017908f, 0.006274f,
                                              -0.011975f, -0.233961f, -0.170792f, -0.255179f, -0.329372f, -0.246594f,
                                              0.008185f, 0.061257f, -0.184882f, -0.026963f, 0.086942f, -0.176101f,
                                              0.325336f, 0.034954f, 0.063210f, -0.309974f, -0.138111f, 0.007556f,
                                              -0.259348f, 0.004047f, -0.092724f, 0.309572f, 0.312339f, 0.161147f,
                                              -0.316051f, 0.166599f, 0.116383f, -0.049004f, -0.093301f, -0.342296f,
                                              0.307445f, -0.305806f, -0.205934f, -0.083518f, 0.300778f, 0.225560f,
                                              -0.327579f, 0.323319f, 0.350282f, -0.083078f, 0.085741f, 0.238602f,
                                              0.016370f, -0.080363f, 0.066285f, -0.244537f, 0.220943f, -0.221814f,
                                              -0.267696f, 0.028362f, 0.336560f, -0.095154f, 0.296983f, -0.148894f,
                                              0.171712f, 0.131055f, 0.350367f, 0.151490f, -0.299399f, 0.269611f,
                                              -0.014017f, 0.229083f, -0.316066f, 0.115106f, -0.186960f, -0.055508f,
                                              0.293270f, 0.095295f, -0.223268f, -0.148063f, -0.295836f, -0.149487f,
                                              0.248994f, -0.160630f, 0.320679f, 0.127871f, 0.039310f, -0.036194f,
                                              -0.233663f, 0.335486f, -0.104857f, -0.055187f, -0.012657f, 0.131262f,
                                              -0.313348f, 0.162709f, 0.116985f, -0.263837f, 0.066075f, -0.338289f,
                                              0.274369f, 0.196197f, 0.103759f, -0.224897f, -0.244354f, 0.091720f,
                                              0.271382f, -0.205587f, -0.021534f, 0.025833f, -0.226594f, 0.343989f,
                                              0.247622f, 0.030369f, 0.092266f, -0.207573f, 0.307776f, -0.128364f,
                                              -0.068126f, 0.021644f, -0.181574f, 0.297832f, 0.220816f, 0.186401f,
                                              0.128671f, 0.282739f, -0.252353f, 0.275549f, -0.349919f, 0.331111f,
                                              0.095210f, 0.193100f, -0.188163f, -0.204274f, 0.077623f, 0.293218f,
                                              -0.204616f, -0.065507f, -0.003398f, -0.301309f, -0.037806f, -0.300699f,
                                              -0.020831f, -0.249405f, -0.141250f, -0.176136f, -0.304141f, 0.085080f,
                                              -0.111655f, -0.254863f, 0.154974f, -0.281073f, 0.048617f, 0.010620f,
                                              -0.090441f, 0.245299f, 0.182319f, -0.129980f, -0.083878f, 0.319522f,
                                              -0.151614f, -0.130455f, -0.055505f, -0.033354f, 0.090969f, 0.171376f,
                                              -0.117560f, 0.177632f, 0.263672f, 0.330078f, 0.245184f, 0.178265f,
                                              -0.077333f, -0.345209f, 0.076466f, 0.024270f, 0.254561f, 0.066402f,
                                              -0.224325f, 0.120349f, -0.121343f, 0.044546f, -0.054005f, -0.312892f,
                                              0.198752f, 0.119275f, -0.029500f, 0.069657f, -0.048522f, 0.318202f,
                                              -0.126363f, 0.047640f, -0.343221f, -0.248505f, 0.335978f, -0.018976f,
                                              0.290766f, 0.276915f, 0.057439f, 0.206736f, 0.006100f, -0.095523f,
                                              -0.048061f, 0.262485f, 0.347647f, 0.223876f, -0.149209f, -0.025787f,
                                              -0.103479f, -0.270352f, 0.275518f, 0.274686f, 0.158047f, 0.205054f,
                                              -0.115222f, 0.141680f, 0.049178f, -0.158417f, -0.242477f, 0.189470f,
                                              -0.313159f, 0.333402f, -0.149650f, 0.123674f, 0.155277f, -0.184427f,
                                              0.286482f, 0.261398f, 0.103375f, -0.027525f, 0.268559f, -0.086400f,
                                              -0.297292f, 0.099228f, 0.311300f, -0.244935f, -0.116378f, 0.348844f,
                                              -0.095433f, -0.141213f, 0.225805f, 0.121356f, -0.203880f, -0.089018f,
                                              0.221538f, 0.082777f, 0.246693f, -0.303965f, -0.139437f, -0.071019f,
                                              0.154347f, -0.076293f, 0.192630f, -0.140596f, 0.250448f, 0.225948f,
                                              -0.186245f, -0.201615f, -0.071148f, -0.200848f, 0.106122f, -0.257729f,
                                              0.093105f, 0.024066f};
            std::vector<float> fc1_bias = {0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f,
                                           0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f,
                                           0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f,
                                           0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f,
                                           0.000000f, 0.000000f, 0.000000f, 0.000000f};
            fc1.set_weights(Tensor4D(1, 32, 16, 1, fc1_weights));
            fc1.set_bias(Tensor4D(1, 32, 1, 1, fc1_bias));

            std::vector<float> fc2_weights = {0.226588f, -0.232472f, 0.231693f, 0.230937f, 0.198427f, 0.053073f,
                                              0.261566f, 0.118889f, -0.115567f, -0.368623f, -0.252809f, 0.364645f,
                                              -0.280462f, 0.066629f, 0.180703f, -0.365833f, -0.230112f, 0.361792f,
                                              -0.362498f, -0.035572f, -0.103294f, 0.197915f, -0.132416f, 0.205962f,
                                              -0.314038f, -0.021480f, -0.087348f, -0.276572f, -0.255770f, 0.242779f,
                                              0.014870f, -0.256420f, -0.050182f, 0.349766f, -0.062861f, -0.154136f,
                                              -0.189362f, -0.119776f, -0.055858f, 0.335718f, -0.183263f, 0.230366f,
                                              0.307581f, -0.298444f, 0.171592f, 0.341894f, -0.053676f, 0.125762f,
                                              -0.356657f, 0.089265f, 0.240842f, -0.363880f, -0.222509f, -0.326402f,
                                              0.345000f, 0.056043f, -0.202639f, 0.176787f, 0.114760f, 0.189232f,
                                              0.241718f, -0.349397f, 0.279491f, -0.029654f, -0.359618f, 0.156485f,
                                              -0.278048f, 0.334692f, 0.028812f, -0.287914f, -0.262408f, 0.370228f,
                                              -0.342396f, 0.077653f, -0.231907f, -0.302656f, 0.193248f, -0.237135f,
                                              -0.152191f, 0.066281f, -0.227477f, 0.290562f, 0.252803f, 0.352370f,
                                              -0.316407f, -0.370199f, 0.158792f, -0.177894f, -0.056796f, -0.082773f,
                                              0.118587f, -0.368433f, 0.170440f, 0.267295f, -0.061913f, 0.031951f,
                                              -0.018072f, -0.127116f, -0.328120f, 0.264984f, -0.177176f, 0.033418f,
                                              0.343860f, 0.027350f, -0.273548f, -0.219736f, 0.173413f, 0.288279f,
                                              0.107783f, -0.298659f, 0.012027f, -0.128644f, -0.198880f, 0.274976f,
                                              0.340241f, -0.127673f, -0.204789f, -0.285971f, -0.335433f, -0.154545f,
                                              0.014378f, -0.362369f, -0.300620f, -0.287667f, 0.363465f, -0.162477f,
                                              0.282896f, 0.142246f, -0.296060f, 0.272099f, 0.215550f, 0.001299f,
                                              -0.014367f, 0.192829f, 0.331891f, 0.115610f, -0.092410f, -0.291064f,
                                              0.000666f, 0.235270f, -0.121715f, -0.023953f, 0.246627f, 0.184230f,
                                              -0.080964f, -0.352341f, 0.204535f, -0.261969f, -0.140920f, 0.176386f,
                                              -0.148331f, 0.262432f, 0.322918f, -0.307104f, -0.166933f, 0.058765f,
                                              -0.355909f, -0.290702f, -0.040168f, -0.212875f, -0.079708f, 0.191894f,
                                              -0.001200f, 0.066474f, -0.005362f, 0.017930f, -0.198696f, 0.028856f,
                                              0.319469f, -0.370995f, 0.227952f, 0.043948f, 0.242707f, 0.351643f,
                                              -0.019943f, -0.013345f, 0.332957f, -0.029811f, 0.118122f, 0.270003f,
                                              0.042542f, -0.157728f, -0.164433f, 0.025850f, -0.109008f, 0.267395f,
                                              -0.181947f, 0.096157f, -0.276263f, 0.234794f, -0.170298f, -0.189964f,
                                              0.166803f, 0.108712f, 0.218693f, -0.074671f, 0.164825f, -0.048542f,
                                              0.192340f, -0.350803f, -0.250426f, 0.166857f, 0.292194f, 0.242659f,
                                              -0.109314f, 0.231713f, 0.357503f, 0.242754f, -0.116407f, -0.065027f,
                                              -0.013931f, 0.009110f, 0.320138f, 0.149864f, 0.335789f, -0.373641f,
                                              -0.211258f, -0.303409f, 0.148557f, 0.328458f, -0.336006f, -0.312552f,
                                              -0.073852f, -0.133237f, 0.207996f, -0.265136f, 0.252482f, 0.300736f,
                                              0.150877f, 0.227577f, -0.083749f, 0.241043f, 0.371207f, 0.284007f,
                                              -0.014728f, -0.302380f, 0.074523f, -0.323997f, -0.103486f, -0.110119f,
                                              -0.304824f, -0.197047f, 0.083187f, -0.176145f, -0.113226f, -0.132506f,
                                              0.090548f, -0.303298f, -0.357959f, 0.197104f, 0.055973f, -0.076649f,
                                              -0.081505f, -0.328947f, -0.299386f, 0.246143f, 0.243832f, -0.248384f,
                                              0.050687f, -0.127684f, 0.264107f, -0.303159f, -0.233271f, -0.080251f,
                                              0.024859f, 0.329800f, -0.017546f, -0.252629f, 0.265531f, -0.146944f,
                                              0.029884f, 0.162603f, -0.350035f, 0.133481f, 0.228192f, -0.013936f,
                                              -0.353600f, 0.361085f, 0.035411f, -0.311879f, 0.244382f, 0.099093f,
                                              -0.332880f, 0.293251f, 0.189422f, 0.297716f, -0.262811f, 0.121241f,
                                              -0.158600f, -0.359208f, -0.002845f, -0.049186f, 0.117167f, -0.020236f,
                                              -0.323507f, -0.154439f, -0.179223f, 0.012522f, 0.349204f, -0.369607f,
                                              -0.141974f, -0.144212f, 0.133335f, -0.258425f, 0.152306f, 0.335311f,
                                              -0.054069f, 0.051146f, 0.194164f, 0.048233f, 0.149762f, -0.356221f,
                                              -0.134432f, -0.165383f, -0.082400f, -0.207478f, 0.046949f, 0.157090f,
                                              0.121062f, -0.097896f};
            std::vector<float> fc2_bias = {0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f,
                                           0.000000f, 0.000000f, 0.000000f};
            fc2.set_weights(Tensor4D(1, 10, 32, 1, fc2_weights));
            fc2.set_bias(Tensor4D(1, 10, 1, 1, fc2_bias));

            std::vector<float> fc3_weights = {0.113796f, -0.061035f, -0.295637f, -0.330307f, 0.193938f, -0.377787f,
                                              -0.100024f, 0.210611f, 0.063914f, 0.140042f, -0.360207f, -0.365517f,
                                              0.386291f, 0.384916f, -0.074306f, 0.096172f, -0.404724f, 0.223767f,
                                              -0.151478f, -0.253830f, 0.302527f, -0.284709f, -0.032027f, -0.283525f,
                                              -0.341205f, -0.026355f, 0.024481f, 0.405433f, 0.322768f, 0.286570f,
                                              -0.162763f, 0.240116f};
            std::vector<float> fc3_bias = {0.000000f};
            fc3.set_weights(Tensor4D(1, 1, 32, 1, fc3_weights));
            fc3.set_bias(Tensor4D(1, 1, 1, 1, fc3_bias));
        }

        std::pair<Tensor4D, Tensor4D> forward(const Tensor4D &x) {
            Tensor4D out = conv1.forward(x);
            out = bn1.forward(out);
            out = ReLULayer().forward(out);
            out = pool.forward(out);
            out = flatten.forward(out);
            out = fc1.forward(out);
            out = ReLULayer().forward(out);

            Tensor4D policy = fc2.forward(out);
            // Note: Apply softmax to policy if needed

            Tensor4D value = fc3.forward(out);
            value = Tanh().forward(value);

            return {policy, value};
        }
    };

} // namespace nnm


/*-- #include "Vector.h" start --*/
/*-- File: main.cpp start --*/
/*-- #include "TicTacToeModel.h" start --*/
#pragma  once

/*-- #include "ConvolutionalLayer.h" start --*/
/*-- #include "BatchNorm2d.h" start --*/
/*-- #include "MaxPoolingLayer.h" start --*/
/*-- #include "FlattenLayer.h" start --*/
/*-- #include "LinearLayer.h" start --*/
/*-- #include "ReLULayer.h" start --*/
/*-- #include "Tanh.h" start --*/
/*-- #include "Tensor4D.h" start --*/
#include <vector>



/*-- #include "Tensor4D.h" start --*/
#include <iostream>
#include <iomanip>

// Function to print the Tic-Tac-Toe board
void printBoard(const nnm::Tensor4D &board) {
    for (size_t i = 0; i < 3; ++i) {
        for (size_t j = 0; j < 3; ++j) {
            char symbol = ' ';
            if (board(0, 0, i, j) == 1) symbol = 'X';
            else if (board(0, 1, i, j) == 1) symbol = 'O';
            std::cout << " " << symbol << " ";
            if (j < 2) std::cout << "|";
        }
        std::cout << std::endl;
        if (i < 2) std::cout << "---+---+---" << std::endl;
    }
    std::cout << std::endl;
}

int main() {
    // Create an instance of the TicTacToeModel
    nnm::TicTacToeModel model;

    // Create a sample Tic-Tac-Toe board state
    // 0 = empty, 1 = X, 2 = O
    nnm::Tensor4D board(1, 3, 3, 3);

    // Set up the board:
    // X |   | O
    // --+---+--
    //   | X |
    // --+---+--
    // O |   | X

    // Channel 0: X positions
    board(0, 0, 0, 0) = 1;
    board(0, 0, 1, 1) = 1;
    board(0, 0, 2, 2) = 1;

    // Channel 1: O positions
    board(0, 1, 0, 2) = 1;
    board(0, 1, 2, 0) = 1;

    // Channel 2: Empty positions (not strictly necessary, but included for completeness)
    board(0, 2, 0, 1) = 1;
    board(0, 2, 1, 0) = 1;
    board(0, 2, 1, 2) = 1;
    board(0, 2, 2, 1) = 1;

    // Print the board
    std::cout << "Current Tic-Tac-Toe board:" << std::endl;
    printBoard(board);

    // Evaluate the board using the model
    auto [policy, value] = model.forward(board);

    // Print the policy (move probabilities)
    std::cout << "Move probabilities:" << std::endl;
    for (size_t i = 0; i < 3; ++i) {
        for (size_t j = 0; j < 3; ++j) {
            std::cout << std::fixed << std::setprecision(4) << policy(0, i * 3 + j, 0, 0) << " ";
        }
        std::cout << std::endl;
    }
    std::cout << std::endl;

    // Print the value (game outcome prediction)
    std::cout << "Predicted game outcome: " << value(0, 0, 0, 0) << std::endl;
    std::cout << "(-1 favors O, +1 favors X)" << std::endl;

    return 0;
}
/*-- File: main.cpp end --*/
